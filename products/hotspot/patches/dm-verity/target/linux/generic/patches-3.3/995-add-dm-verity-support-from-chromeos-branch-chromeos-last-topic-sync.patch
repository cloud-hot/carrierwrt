Index: linux-3.3.8/Documentation/device-mapper/dm-bht.txt
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.3.8/Documentation/device-mapper/dm-bht.txt	2014-02-09 08:02:38.855062478 +0000
@@ -0,0 +1,73 @@
+dm-bht
+======
+
+dm-bht provides a block hash tree implementation.  The use of dm-bht allows
+for integrity checking of a given block device without reading the entire
+set of blocks into memory before use.
+
+In particular, dm-bht supplies an interface for creating and verifying a tree
+of cryptographic digests with any algorithm supported by the kernel crypto API.
+
+The code is meant to be usable from user-space for creation and verification as
+well as directly from a Device-Mapper target.  The `verity' target is the
+motivating example.
+
+
+Theory of operation
+===================
+
+dm-bht is logically comprised of multiple nodes organized in a tree-like
+structure.  Each node in the tree is a cryptographic hash.  If it is a leaf
+node, the hash is of some block data on disk.  If it is an intermediary node,
+then the hash is of a number of child nodes.
+
+dm-bht has a given depth starting at 1 (ignoring the root node).  Each level in
+the tree is concretely made up of dm_bht_entry structs.  Each entry in the tree
+is a collection of neighboring nodes that fit in one page-sized block.  The
+number is determined based on PAGE_SIZE and the size of the selected
+cryptographic digest algorithm.  The hashes are linearly ordered in this entry
+and any unaligned trailing space is ignored but included when calculating the
+parent node.
+
+The tree looks something like:
+
+depth = 2, alg= sha256, num_blocks = 32767
+                                 [   root    ]
+                                /    . . .    \
+                     [entry_0]                 [entry_1]
+                    /  . . .  \                 . . .   \
+         [entry_0_0]   . . .  [entry_0_127]    . . . .  [entry_1_127]
+           / ... \             /   . . .  \             /           \
+     blk_0 ... blk_127  blk_16256   blk_16383      blk_32640 . . . blk_32767
+
+root is treated independently from the depth and the blocks are expected to
+be hashed and supplied to the dm-bht.  hash blocks that make up the entry
+contents are expected to be read from disk.
+
+dm-bht does not handle I/O directly but instead expects the consumer to
+supply callbacks.  The read callback will always receive a page-align value
+to pass to the block device layer to read in a hash value.
+
+Usage
+=====
+
+The API provides mechanisms for reading and verifying a tree as well as
+creating and modifying the tree.  These two code paths were not meant to be
+used in parallel and modify the atomic entry values in incompatible ways.
+Where possible, tree creation and modification should be handled independently
+from tree verification.
+
+When reading, all required data for the hash tree should be populated for a
+block before attempting a verify.  This can be done by calling
+dm_bht_populate().  When all data is ready, a call to dm_bht_verify_block()
+with the expected hash value will perform both the direct block hash check and
+the hashes of the parent and neighboring nodes where needed to ensure validity
+up to the root hash.  Note, dm_bht_set_root_hexdigest() should be called before
+any verification attempts occur.
+
+When updating the tree, all block hashes should be stored with
+dm_bht_store_block().  Once all hashes are stored, a call to dm_bht_compute()
+will initiate a full tree update by walking all of the blocks of hashes
+starting at the leaf nodes and computing upward to the root node.  On
+completion, dm_bht_sync() may be called to write the tree to disk (or wherever
+the callback writes to).
Index: linux-3.3.8/Documentation/device-mapper/dm-verity.txt
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.3.8/Documentation/device-mapper/dm-verity.txt	2014-02-09 08:02:38.855062478 +0000
@@ -0,0 +1,71 @@
+dm-verity
+==========
+
+Device-Mapper's "verity" target provides transparent integrity checking of
+block devices using a cryptographic digest provided by the kernel crypto API.
+This target is read-only.
+
+Parameters: <device path> <hash device path> <tree depth> <alg> <parent-hash>
+
+<device path>
+    This is the device that is going to be integrity checked.  It may be
+    a subset of the full device as specified to dmsetup (start sector and count)
+    It may be specified as a path, like /dev/sdaX, or a device number,
+    <major>:<minor>.
+
+<hash device path>
+    This is the device that that supplies the dm-bht hash data.  It may be
+    specified similarly to the device path and may be the same device.  If the
+    same device is used, the hash offset should be outside of the dm-verity
+    configured device size.
+
+<tree depth>
+    The tree depth determines how many levels of hashes are used when building
+    the tree of hashes.  The root of the tree not included and the leaves of
+    the tree are the hashes of the blocks on disk.
+
+<alg>
+    The cryptographic hash algorithm used for this device.  This should
+    be the name of the algorithm, like "sha1".
+
+<root hash>
+    The hexadecimal encoding of the cryptographic hash of all of the
+    neighboring nodes at the first level of the tree.  This hash should be
+    trusted as there is no other authenticity beyond this point.
+
+
+Theory of operation
+===================
+
+dm-verity is meant to be setup as part of a verified boot path.  This
+may be anything ranging from a boot using tboot or trustedgrub to just
+booting from a known-good device (like a USB drive or CD).
+
+When a dm-verity device is configured, it is expected that the caller
+has been authenticated in some way (cryptographic signatures, etc).
+After instantiation, all hashes will be verified on-demand during
+disk access.  If they cannot be verified up to the root node of the
+tree, the root hash, then the I/O will fail.  This should identify
+tampering with any data on the device and the hash data.
+
+Cryptographic hashes are used to assert the integrity of the device on a
+per-block basis.  This allows for a lightweight hash computation on first read
+into the page cache.  Block hashes are stored linearly aligned to the nearest
+block the size of a page.
+
+For more information on the hashing process, see dm-bht.txt.
+
+
+Example
+=======
+
+Setup a device;
+[[
+  dmsetup create vroot --table \
+    "0 204800 verity /dev/sda1 /dev/sda2 0 3 sha1 "\
+    "9f74809a2ee7607b16fcc70d9399a4de9725a727"
+]]
+
+A command line tool is available to compute the hash tree and return the
+root hash value.
+  http://git.chromium.org/cgi-bin/gitweb.cgi?p=dm-verity.git;a=tree
Index: linux-3.3.8/drivers/md/Kconfig
===================================================================
--- linux-3.3.8.orig/drivers/md/Kconfig	2012-06-01 07:16:13.000000000 +0000
+++ linux-3.3.8/drivers/md/Kconfig	2014-02-09 08:02:38.859062638 +0000
@@ -349,6 +349,40 @@
 
 	  If unsure, say N.
 
+config DM_BHT
+        tristate "Block hash tree support"
+        select CRYPTO
+        select CRYPTO_HASH
+        ---help---
+          Include support for device-mapper devices to use a block hash
+          tree for managing data integrity checks in a scalable way.
+
+          Targets that use this functionality should include it
+          automatically.
+
+          If unsure, say N.
+
+config DM_VERITY
+        tristate "Verity target support"
+        depends on BLK_DEV_DM
+        select DM_BHT
+        select CRYPTO
+        select CRYPTO_HASH
+        ---help---
+          This device-mapper target allows you to create a device that
+          transparently integrity checks the data on it. You'll need to
+          activate the digests you're going to use in the cryptoapi
+          configuration.
+
+          Information on how to use dm-verity can be found on
+
+          <http://dev.chromium.org/chromium-os/chromiumos-design-docs/verified-boot>
+
+          To compile this code as a module, choose M here: the module will
+          be called dm-verity.
+
+          If unsure, say N.
+
 config DM_DELAY
 	tristate "I/O delaying target (EXPERIMENTAL)"
 	depends on BLK_DEV_DM && EXPERIMENTAL
Index: linux-3.3.8/drivers/md/Makefile
===================================================================
--- linux-3.3.8.orig/drivers/md/Makefile	2012-06-01 07:16:13.000000000 +0000
+++ linux-3.3.8/drivers/md/Makefile	2014-02-09 08:02:38.863062799 +0000
@@ -42,6 +42,9 @@
 obj-$(CONFIG_DM_ZERO)		+= dm-zero.o
 obj-$(CONFIG_DM_RAID)	+= dm-raid.o
 obj-$(CONFIG_DM_THIN_PROVISIONING)	+= dm-thin-pool.o
+obj-$(CONFIG_DM_BHT)		+= dm-bht.o
+obj-$(CONFIG_DM_VERITY)		+= dm-verity.o
+obj-$(CONFIG_DM_VERITY_CHROMEOS)	+= dm-verity-chromeos.o
 
 ifeq ($(CONFIG_DM_UEVENT),y)
 dm-mod-objs			+= dm-uevent.o
Index: linux-3.3.8/drivers/md/dm-bht.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.3.8/drivers/md/dm-bht.c	2014-02-09 08:02:38.863062799 +0000
@@ -0,0 +1,980 @@
+ /*
+ * Copyright (C) 2010 The Chromium OS Authors <chromium-os-dev@chromium.org>
+ *
+ * Device-Mapper block hash tree interface.
+ * See Documentation/device-mapper/dm-bht.txt for details.
+ *
+ * This file is released under the GPL.
+ */
+
+#include <asm/atomic.h>
+#include <asm/page.h>
+#include <linux/bitops.h>  /* for fls() */
+#include <linux/bug.h>
+#include <linux/cpumask.h>  /* nr_cpu_ids */
+/* #define CONFIG_DM_DEBUG 1 */
+#include <linux/device-mapper.h>
+#include <linux/err.h>
+#include <linux/errno.h>
+#include <linux/gfp.h>
+#include <linux/dm-bht.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mm_types.h>
+#include <linux/scatterlist.h>
+#include <linux/slab.h>  /* k*alloc */
+#include <linux/string.h>  /* memset */
+
+#define DM_MSG_PREFIX "dm bht"
+
+/* For sector formatting. */
+#if defined(_LP64) || defined(__LP64__) || __BITS_PER_LONG == 64
+#define __PRIS_PREFIX "z"
+#else
+#define __PRIS_PREFIX "ll"
+#endif
+#define PRIu64 __PRIS_PREFIX "u"
+
+
+/*-----------------------------------------------
+ * Utilities
+ *-----------------------------------------------*/
+
+static u8 from_hex(u8 ch)
+{
+	if ((ch >= '0') && (ch <= '9'))
+		return ch - '0';
+	if ((ch >= 'a') && (ch <= 'f'))
+		return ch - 'a' + 10;
+	if ((ch >= 'A') && (ch <= 'F'))
+		return ch - 'A' + 10;
+	return -1;
+}
+
+/**
+ * dm_bht_bin_to_hex - converts a binary stream to human-readable hex
+ * @binary:	a byte array of length @binary_len
+ * @hex:	a byte array of length @binary_len * 2 + 1
+ */
+static void dm_bht_bin_to_hex(u8 *binary, u8 *hex, unsigned int binary_len)
+{
+	while (binary_len-- > 0) {
+		sprintf((char *__restrict__)hex, "%02hhx", (int)*binary);
+		hex += 2;
+		binary++;
+	}
+}
+
+/**
+ * dm_bht_hex_to_bin - converts a hex stream to binary
+ * @binary:	a byte array of length @binary_len
+ * @hex:	a byte array of length @binary_len * 2 + 1
+ */
+static void dm_bht_hex_to_bin(u8 *binary, const u8 *hex,
+			      unsigned int binary_len)
+{
+	while (binary_len-- > 0) {
+		*binary = from_hex(*(hex++));
+		*binary *= 16;
+		*binary += from_hex(*(hex++));
+		binary++;
+	}
+}
+
+static void dm_bht_log_mismatch(struct dm_bht *bht, u8 *given, u8 *computed)
+{
+	u8 given_hex[DM_BHT_MAX_DIGEST_SIZE * 2 + 1];
+	u8 computed_hex[DM_BHT_MAX_DIGEST_SIZE * 2 + 1];
+	dm_bht_bin_to_hex(given, given_hex, bht->digest_size);
+	dm_bht_bin_to_hex(computed, computed_hex, bht->digest_size);
+	DMERR_LIMIT("%s != %s", given_hex, computed_hex);
+}
+
+/* Used for turning verifiers into computers */
+typedef int (*dm_bht_compare_cb)(struct dm_bht *, u8 *, u8 *);
+
+/**
+ * dm_bht_compute_hash: hashes a page of data
+ */
+static int dm_bht_compute_hash(struct dm_bht *bht, struct page *pg,
+			       unsigned int offset, u8 *digest)
+{
+	struct hash_desc *hash_desc = &bht->hash_desc[smp_processor_id()];
+	struct scatterlist sg;
+
+	sg_init_table(&sg, 1);
+	sg_set_page(&sg, pg, PAGE_SIZE, offset);
+	/* Note, this is synchronous. */
+	if (crypto_hash_init(hash_desc)) {
+		DMCRIT("failed to reinitialize crypto hash (proc:%d)",
+			smp_processor_id());
+		return -EINVAL;
+	}
+	if (crypto_hash_update(hash_desc, &sg, PAGE_SIZE)) {
+		DMCRIT("crypto_hash_update failed");
+		return -EINVAL;
+	}
+	if (bht->have_salt) {
+		sg_set_buf(&sg, bht->salt, sizeof(bht->salt));
+		if (crypto_hash_update(hash_desc, &sg, sizeof(bht->salt))) {
+			DMCRIT("crypto_hash_update failed");
+			return -EINVAL;
+		}
+	}
+	if (crypto_hash_final(hash_desc, digest)) {
+		DMCRIT("crypto_hash_final failed");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static __always_inline struct dm_bht_level *dm_bht_get_level(struct dm_bht *bht,
+							     int depth)
+{
+	return &bht->levels[depth];
+}
+
+static __always_inline unsigned int dm_bht_get_level_shift(struct dm_bht *bht,
+							   int depth)
+{
+	return (bht->depth - depth) * bht->node_count_shift;
+}
+
+/* For the given depth, this is the entry index.  At depth+1 it is the node
+ * index for depth.
+ */
+static __always_inline unsigned int dm_bht_index_at_level(struct dm_bht *bht,
+							  int depth,
+							  unsigned int leaf)
+{
+	return leaf >> dm_bht_get_level_shift(bht, depth);
+}
+
+static __always_inline u8 *dm_bht_node(struct dm_bht *bht,
+				       struct dm_bht_entry *entry,
+				       unsigned int node_index)
+{
+	return &entry->nodes[node_index * bht->digest_size];
+}
+
+static inline struct dm_bht_entry *dm_bht_get_entry(struct dm_bht *bht,
+						    int depth,
+						    unsigned int block)
+{
+	unsigned int index = dm_bht_index_at_level(bht, depth, block);
+	struct dm_bht_level *level = dm_bht_get_level(bht, depth);
+
+	BUG_ON(index >= level->count);
+
+	return &level->entries[index];
+}
+
+static inline u8 *dm_bht_get_node(struct dm_bht *bht,
+				  struct dm_bht_entry *entry,
+				  int depth,
+				  unsigned int block)
+{
+	unsigned int index = dm_bht_index_at_level(bht, depth, block);
+
+	return dm_bht_node(bht, entry, index % bht->node_count);
+}
+
+
+/*-----------------------------------------------
+ * Implementation functions
+ *-----------------------------------------------*/
+
+static int dm_bht_initialize_entries(struct dm_bht *bht);
+
+static int dm_bht_read_callback_stub(void *ctx, sector_t start, u8 *dst,
+				     sector_t count,
+				     struct dm_bht_entry *entry);
+static int dm_bht_write_callback_stub(void *ctx, sector_t start,
+				      u8 *dst, sector_t count,
+				      struct dm_bht_entry *entry);
+
+/**
+ * dm_bht_create - prepares @bht for us
+ * @bht:	pointer to a dm_bht_create()d bht
+ * @depth:	tree depth without the root; including block hashes
+ * @block_count:the number of block hashes / tree leaves
+ * @alg_name:	crypto hash algorithm name
+ *
+ * Returns 0 on success.
+ *
+ * Callers can offset into devices by storing the data in the io callbacks.
+ * TODO(wad) bust up into smaller helpers
+ */
+int dm_bht_create(struct dm_bht *bht, unsigned int block_count,
+		  const char *alg_name)
+{
+	int status = 0;
+	int cpu = 0;
+
+	bht->have_salt = false;
+
+	/* Setup the hash first. Its length determines much of the bht layout */
+	for (cpu = 0; cpu < nr_cpu_ids; ++cpu) {
+		bht->hash_desc[cpu].tfm = crypto_alloc_hash(alg_name, 0, 0);
+		if (IS_ERR(bht->hash_desc[cpu].tfm)) {
+			DMERR("failed to allocate crypto hash '%s'", alg_name);
+			status = -ENOMEM;
+			bht->hash_desc[cpu].tfm = NULL;
+			goto bad_hash_alg;
+		}
+	}
+	bht->digest_size = crypto_hash_digestsize(bht->hash_desc[0].tfm);
+	/* We expect to be able to pack >=2 hashes into a page */
+	if (PAGE_SIZE / bht->digest_size < 2) {
+		DMERR("too few hashes fit in a page");
+		status = -EINVAL;
+		goto bad_digest_len;
+	}
+
+	if (bht->digest_size > DM_BHT_MAX_DIGEST_SIZE) {
+		DMERR("DM_BHT_MAX_DIGEST_SIZE too small for chosen digest");
+		status = -EINVAL;
+		goto bad_digest_len;
+	}
+
+	/* Configure the tree */
+	bht->block_count = block_count;
+	DMDEBUG("Setting block_count %u", block_count);
+	if (block_count == 0) {
+		DMERR("block_count must be non-zero");
+		status = -EINVAL;
+		goto bad_block_count;
+	}
+
+	/* Each dm_bht_entry->nodes is one page.  The node code tracks
+	 * how many nodes fit into one entry where a node is a single
+	 * hash (message digest).
+	 */
+	bht->node_count_shift = fls(PAGE_SIZE / bht->digest_size) - 1;
+	/* Round down to the nearest power of two.  This makes indexing
+	 * into the tree much less painful.
+	 */
+	bht->node_count = 1 << bht->node_count_shift;
+
+	/* This is unlikely to happen, but with 64k pages, who knows. */
+	if (bht->node_count > UINT_MAX / bht->digest_size) {
+		DMERR("node_count * hash_len exceeds UINT_MAX!");
+		status = -EINVAL;
+		goto bad_node_count;
+	}
+
+	bht->depth = DIV_ROUND_UP(fls(block_count - 1), bht->node_count_shift);
+	DMDEBUG("Setting depth to %d.", bht->depth);
+
+	/* Ensure that we can safely shift by this value. */
+	if (bht->depth * bht->node_count_shift >= sizeof(unsigned int) * 8) {
+		DMERR("specified depth and node_count_shift is too large");
+		status = -EINVAL;
+		goto bad_node_count;
+	}
+
+	/* Allocate levels. Each level of the tree may have an arbitrary number
+	 * of dm_bht_entry structs.  Each entry contains node_count nodes.
+	 * Each node in the tree is a cryptographic digest of either node_count
+	 * nodes on the subsequent level or of a specific block on disk.
+	 */
+	bht->levels = (struct dm_bht_level *)
+			kcalloc(bht->depth,
+				sizeof(struct dm_bht_level), GFP_KERNEL);
+	if (!bht->levels) {
+		DMERR("failed to allocate tree levels");
+		status = -ENOMEM;
+		goto bad_level_alloc;
+	}
+
+	/* Setup callback stubs */
+	bht->read_cb = &dm_bht_read_callback_stub;
+	bht->write_cb = &dm_bht_write_callback_stub;
+
+	status = dm_bht_initialize_entries(bht);
+	if (status)
+		goto bad_entries_alloc;
+
+	/* We compute depth such that there is only be 1 block at level 0. */
+	BUG_ON(bht->levels[0].count != 1);
+
+	return 0;
+
+bad_entries_alloc:
+	while (bht->depth-- > 0)
+		kfree(bht->levels[bht->depth].entries);
+	kfree(bht->levels);
+bad_node_count:
+bad_level_alloc:
+bad_block_count:
+bad_digest_len:
+bad_hash_alg:
+	for (cpu = 0; cpu < nr_cpu_ids; ++cpu)
+		if (bht->hash_desc[cpu].tfm)
+			crypto_free_hash(bht->hash_desc[cpu].tfm);
+	return status;
+}
+EXPORT_SYMBOL(dm_bht_create);
+
+static int dm_bht_initialize_entries(struct dm_bht *bht)
+{
+	/* The last_index represents the index into the last
+	 * block digest that will be stored in the tree.  By walking the
+	 * tree with that index, it is possible to compute the total number
+	 * of entries needed at each level in the tree.
+	 *
+	 * Since each entry will contain up to |node_count| nodes of the tree,
+	 * it is possible that the last index may not be at the end of a given
+	 * entry->nodes.  In that case, it is assumed the value is padded.
+	 *
+	 * Note, we treat both the tree root (1 hash) and the tree leaves
+	 * independently from the bht data structures.  Logically, the root is
+	 * depth=-1 and the block layer level is depth=bht->depth
+	 */
+	unsigned int last_index = ALIGN(bht->block_count, bht->node_count) - 1;
+	unsigned int total_entries = 0;
+	struct dm_bht_level *level = NULL;
+	int depth;
+
+	/* check that the largest level->count can't result in an int overflow
+	 * on allocation or sector calculation.
+	 */
+	if (((last_index >> bht->node_count_shift) + 1) >
+	    UINT_MAX / max((unsigned int)sizeof(struct dm_bht_entry),
+			   (unsigned int)to_sector(PAGE_SIZE))) {
+		DMCRIT("required entries %u is too large",
+		       last_index + 1);
+		return -EINVAL;
+	}
+
+	/* Track the current sector location for each level so we don't have to
+	 * compute it during traversals.
+	 */
+	bht->sectors = 0;
+	for (depth = 0; depth < bht->depth; ++depth) {
+		level = dm_bht_get_level(bht, depth);
+		level->count = dm_bht_index_at_level(bht, depth,
+						     last_index) + 1;
+		DMDEBUG("depth: %d entries: %u", depth, level->count);
+		/* TODO(wad) consider the case where the data stored for each
+		 * level is done with contiguous pages (instead of using
+		 * entry->nodes) and the level just contains two bitmaps:
+		 * (a) which pages have been loaded from disk
+		 * (b) which specific nodes have been verified.
+		 */
+		level->entries = (struct dm_bht_entry *)
+				 kcalloc(level->count,
+					 sizeof(struct dm_bht_entry),
+					 GFP_KERNEL);
+		if (!level->entries) {
+			DMERR("failed to allocate entries for depth %d",
+			      bht->depth);
+			/* let the caller clean up the mess */
+			return -ENOMEM;
+		}
+		total_entries += level->count;
+		level->sector = bht->sectors;
+		/* number of sectors per entry * entries at this level */
+		bht->sectors += level->count * to_sector(PAGE_SIZE);
+		/* not ideal, but since unsigned overflow behavior is defined */
+		if (bht->sectors < level->sector) {
+			DMCRIT("level sector calculation overflowed");
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static int dm_bht_read_callback_stub(void *ctx, sector_t start, u8 *dst,
+				     sector_t count, struct dm_bht_entry *entry)
+{
+	DMCRIT("dm_bht_read_callback_stub called!");
+	dm_bht_read_completed(entry, -EIO);
+	return -EIO;
+}
+
+static int dm_bht_write_callback_stub(void *ctx, sector_t start,
+				      u8 *dst, sector_t count,
+				      struct dm_bht_entry *entry)
+{
+	DMCRIT("dm_bht_write_callback_stub called!");
+	dm_bht_write_completed(entry, -EIO);
+	return -EIO;
+}
+
+/**
+ * dm_bht_read_completed
+ * @entry:	pointer to the entry that's been loaded
+ * @status:	I/O status. Non-zero is failure.
+ * MUST always be called after a read_cb completes.
+ */
+void dm_bht_read_completed(struct dm_bht_entry *entry, int status)
+{
+	if (status) {
+		/* TODO(wad) add retry support */
+		DMCRIT("an I/O error occurred while reading entry");
+		atomic_set(&entry->state, DM_BHT_ENTRY_ERROR_IO);
+		/* entry->nodes will be freed later */
+		return;
+	}
+	BUG_ON(atomic_read(&entry->state) != DM_BHT_ENTRY_PENDING);
+	atomic_set(&entry->state, DM_BHT_ENTRY_READY);
+}
+EXPORT_SYMBOL(dm_bht_read_completed);
+
+/**
+ * dm_bht_write_completed
+ * @entry:	pointer to the entry that's been loaded
+ * @status:	I/O status. Non-zero is failure.
+ * Should be called after a write_cb completes. Currently only catches
+ * errors which more writers don't care about.
+ */
+void dm_bht_write_completed(struct dm_bht_entry *entry, int status)
+{
+	if (status) {
+		DMCRIT("an I/O error occurred while writing entry");
+		atomic_set(&entry->state, DM_BHT_ENTRY_ERROR_IO);
+		/* entry->nodes will be freed later */
+		return;
+	}
+}
+EXPORT_SYMBOL(dm_bht_write_completed);
+
+/* dm_bht_verify_path
+ * Verifies the path. Returns 0 on ok.
+ */
+static int dm_bht_verify_path(struct dm_bht *bht, unsigned int block,
+			      struct page *pg, unsigned int offset)
+{
+	int depth = bht->depth;
+	u8 digest[DM_BHT_MAX_DIGEST_SIZE];
+	struct dm_bht_entry *entry;
+	u8 *node;
+	int state;
+
+	do {
+		/* Need to check that the hash of the current block is accurate
+		 * in its parent.
+		 */
+		entry = dm_bht_get_entry(bht, depth - 1, block);
+		state = atomic_read(&entry->state);
+		/* This call is only safe if all nodes along the path
+		 * are already populated (i.e. READY) via dm_bht_populate.
+		 */
+		BUG_ON(state < DM_BHT_ENTRY_READY);
+		node = dm_bht_get_node(bht, entry, depth, block);
+
+		if (dm_bht_compute_hash(bht, pg, offset, digest) ||
+		    memcmp(digest, node, bht->digest_size))
+			goto mismatch;
+
+		/* Keep the containing block of hashes to be verified in the
+		 * next pass.
+		 */
+		pg = virt_to_page(entry->nodes);
+		offset = 0;
+	} while (--depth > 0 && state != DM_BHT_ENTRY_VERIFIED);
+
+	if (depth == 0 && state != DM_BHT_ENTRY_VERIFIED) {
+		if (dm_bht_compute_hash(bht, pg, offset, digest) ||
+		    memcmp(digest, bht->root_digest, bht->digest_size))
+			goto mismatch;
+		atomic_set(&entry->state, DM_BHT_ENTRY_VERIFIED);
+	}
+
+	/* Mark path to leaf as verified. */
+	for (depth++; depth < bht->depth; depth++) {
+		entry = dm_bht_get_entry(bht, depth, block);
+		/* At this point, entry can only be in VERIFIED or READY state.
+		 * So it is safe to use atomic_set instead of atomic_cmpxchg.
+		 */
+		atomic_set(&entry->state, DM_BHT_ENTRY_VERIFIED);
+	}
+
+	DMDEBUG("verify_path: node %u is verified to root", block);
+	return 0;
+
+mismatch:
+	DMERR_LIMIT("verify_path: failed to verify hash (d=%d,bi=%u)",
+		    depth, block);
+	dm_bht_log_mismatch(bht, node, digest);
+	return DM_BHT_ENTRY_ERROR_MISMATCH;
+}
+
+/**
+ * dm_bht_store_block - sets a given block's hash in the tree
+ * @bht:	pointer to a dm_bht_create()d bht
+ * @block:	numeric index of the block in the tree
+ * @digest:	array of u8s containing the digest of length @bht->digest_size
+ *
+ * Returns 0 on success, >0 when data is pending, and <0 when a IO or other
+ * error has occurred.
+ *
+ * If the containing entry in the tree is unallocated, it will allocate memory
+ * and mark the entry as ready.  All other block entries will be 0s.  This
+ * function is not safe for simultaneous use when verifying data and should not
+ * be used if the @bht is being accessed by any other functions in any other
+ * threads/processes.
+ *
+ * It is expected that virt_to_page will work on |block_data|.
+ */
+int dm_bht_store_block(struct dm_bht *bht, unsigned int block,
+		       u8 *block_data)
+{
+	int depth;
+	unsigned int index;
+	unsigned int node_index;
+	struct dm_bht_entry *entry;
+	struct dm_bht_level *level;
+	int state;
+	struct page *node_page = NULL;
+
+	/* Look at the last level of nodes above the leaves (data blocks) */
+	depth = bht->depth - 1;
+
+	/* Index into the level */
+	level = dm_bht_get_level(bht, depth);
+	index = dm_bht_index_at_level(bht, depth, block);
+	/* Grab the node index into the current entry by getting the
+	 * index at the leaf-level.
+	 */
+	node_index = dm_bht_index_at_level(bht, depth + 1, block) %
+		     bht->node_count;
+	entry = &level->entries[index];
+
+	DMDEBUG("Storing block %u in d=%d,ei=%u,ni=%u,s=%d",
+		block, depth, index, node_index,
+		atomic_read(&entry->state));
+
+	state = atomic_cmpxchg(&entry->state,
+			       DM_BHT_ENTRY_UNALLOCATED,
+			       DM_BHT_ENTRY_PENDING);
+	/* !!! Note. It is up to the users of the update interface to
+	 *     ensure the entry data is fully populated prior to use.
+	 *     The number of updated entries is NOT tracked.
+	 */
+	if (state == DM_BHT_ENTRY_UNALLOCATED) {
+		node_page = alloc_page(GFP_KERNEL);
+		if (!node_page) {
+			atomic_set(&entry->state, DM_BHT_ENTRY_ERROR);
+			return -ENOMEM;
+		}
+		entry->nodes = page_address(node_page);
+		memset(entry->nodes, 0, PAGE_SIZE);
+		/* TODO(wad) could expose this to the caller to that they
+		 * can transition from unallocated to ready manually.
+		 */
+		atomic_set(&entry->state, DM_BHT_ENTRY_READY);
+	} else if (state <= DM_BHT_ENTRY_ERROR) {
+		DMCRIT("leaf entry for block %u is invalid",
+		      block);
+		return state;
+	} else if (state == DM_BHT_ENTRY_PENDING) {
+		DMERR("leaf data is pending for block %u", block);
+		return 1;
+	}
+
+	dm_bht_compute_hash(bht, virt_to_page(block_data), 0,
+			    dm_bht_node(bht, entry, node_index));
+	return 0;
+}
+EXPORT_SYMBOL(dm_bht_store_block);
+
+/**
+ * dm_bht_zeroread_callback - read callback which always returns 0s
+ * @ctx:	ignored
+ * @start:	ignored
+ * @data:	buffer to write 0s to
+ * @count:	number of sectors worth of data to write
+ * @complete_ctx: opaque context for @completed
+ * @completed: callback to confirm end of data read
+ *
+ * Always returns 0.
+ *
+ * Meant for use by dm_compute() callers.  It allows dm_populate to
+ * be used to pre-fill a tree with zeroed out entry nodes.
+ */
+int dm_bht_zeroread_callback(void *ctx, sector_t start, u8 *dst,
+			     sector_t count, struct dm_bht_entry *entry)
+{
+	memset(dst, 0, to_bytes(count));
+	dm_bht_read_completed(entry, 0);
+	return 0;
+}
+EXPORT_SYMBOL(dm_bht_zeroread_callback);
+
+/**
+ * dm_bht_compute - computes and updates all non-block-level hashes in a tree
+ * @bht:	pointer to a dm_bht_create()d bht
+ * @read_cb_ctx:opaque read_cb context for all I/O on this call
+ *
+ * Returns 0 on success, >0 when data is pending, and <0 when a IO or other
+ * error has occurred.
+ *
+ * Walks the tree and computes the hashes at each level from the
+ * hashes below. This can only be called once per tree creation
+ * since it will mark entries verified. Expects dm_bht_populate() to
+ * correctly populate the tree from the read_callback_stub.
+ *
+ * This function should not be used when verifying the same tree and
+ * should not be used with multiple simultaneous operators on @bht.
+ */
+int dm_bht_compute(struct dm_bht *bht, void *read_cb_ctx)
+{
+	int depth, r = 0;
+
+	for (depth = bht->depth - 2; depth >= 0; depth--) {
+		struct dm_bht_level *level = dm_bht_get_level(bht, depth);
+		struct dm_bht_level *child_level = level + 1;
+		struct dm_bht_entry *entry = level->entries;
+		struct dm_bht_entry *child = child_level->entries;
+		unsigned int i, j;
+
+		for (i = 0; i < level->count; i++, entry++) {
+			unsigned int count = bht->node_count;
+			struct page *pg;
+
+			pg = alloc_page(GFP_NOIO);
+			if (!pg) {
+				DMCRIT("an error occurred while reading entry");
+				goto out;
+			}
+
+			entry->nodes = page_address(pg);
+			memset(entry->nodes, 0, PAGE_SIZE);
+			atomic_set(&entry->state, DM_BHT_ENTRY_READY);
+
+			if (i == (level->count - 1))
+				count = child_level->count % bht->node_count;
+			if (count == 0)
+				count = bht->node_count;
+			for (j = 0; j < count; j++, child++) {
+				struct page *pg = virt_to_page(child->nodes);
+				u8 *digest = dm_bht_node(bht, entry, j);
+
+				r = dm_bht_compute_hash(bht, pg, 0, digest);
+				if (r) {
+					DMERR("Failed to update (d=%d,i=%u)",
+					      depth, i);
+					goto out;
+				}
+			}
+		}
+	}
+	r = dm_bht_compute_hash(bht,
+				virt_to_page(bht->levels[0].entries->nodes),
+				0, bht->root_digest);
+	if (r)
+		DMERR("Failed to update root hash");
+
+out:
+	return r;
+}
+EXPORT_SYMBOL(dm_bht_compute);
+
+/**
+ * dm_bht_sync - writes the tree in memory to disk
+ * @bht:	pointer to a dm_bht_create()d bht
+ * @write_ctx:	callback context for writes issued
+ *
+ * Since all entry nodes are PAGE_SIZE, the data will be pre-aligned and
+ * padded.
+ */
+int dm_bht_sync(struct dm_bht *bht, void *write_cb_ctx)
+{
+	int depth;
+	int ret = 0;
+	int state;
+	sector_t sector;
+	struct dm_bht_level *level;
+	struct dm_bht_entry *entry;
+	struct dm_bht_entry *entry_end;
+
+	for (depth = 0; depth < bht->depth; ++depth) {
+		level = dm_bht_get_level(bht, depth);
+		entry_end = level->entries + level->count;
+		sector = level->sector;
+		for (entry = level->entries; entry < entry_end; ++entry) {
+			state = atomic_read(&entry->state);
+			if (state <= DM_BHT_ENTRY_PENDING) {
+				DMERR("At depth %d, entry %lu is not ready",
+				      depth,
+				      (unsigned long)(entry - level->entries));
+				return state;
+			}
+			ret = bht->write_cb(write_cb_ctx,
+					    sector,
+					    entry->nodes,
+					    to_sector(PAGE_SIZE),
+					    entry);
+			if (ret) {
+				DMCRIT("an error occurred writing entry %lu",
+				      (unsigned long)(entry - level->entries));
+				return ret;
+			}
+			sector += to_sector(PAGE_SIZE);
+		}
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(dm_bht_sync);
+
+/**
+ * dm_bht_is_populated - check that entries from disk needed to verify a given
+ *                       block are all ready
+ * @bht:	pointer to a dm_bht_create()d bht
+ * @block:	specific block data is expected from
+ *
+ * Callers may wish to call dm_bht_is_populated() when checking an io
+ * for which entries were already pending.
+ */
+bool dm_bht_is_populated(struct dm_bht *bht, unsigned int block)
+{
+	int depth;
+
+	for (depth = bht->depth - 1; depth >= 0; depth--) {
+		struct dm_bht_entry *entry = dm_bht_get_entry(bht, depth,
+							      block);
+		if (atomic_read(&entry->state) < DM_BHT_ENTRY_READY)
+			return false;
+	}
+
+	return true;
+}
+EXPORT_SYMBOL(dm_bht_is_populated);
+
+/**
+ * dm_bht_populate - reads entries from disk needed to verify a given block
+ * @bht:	pointer to a dm_bht_create()d bht
+ * @ctx:        context used for all read_cb calls on this request
+ * @block:	specific block data is expected from
+ *
+ * Returns negative value on error. Returns 0 on success.
+ */
+int dm_bht_populate(struct dm_bht *bht, void *ctx,
+		    unsigned int block)
+{
+	int depth;
+	int state = 0;
+
+	BUG_ON(block >= bht->block_count);
+
+	DMDEBUG("dm_bht_populate(%u)", block);
+
+	for (depth = bht->depth - 1; depth >= 0; --depth) {
+		struct dm_bht_level *level;
+		struct dm_bht_entry *entry;
+		unsigned int index;
+		struct page *pg;
+
+		entry = dm_bht_get_entry(bht, depth, block);
+		state = atomic_cmpxchg(&entry->state,
+				       DM_BHT_ENTRY_UNALLOCATED,
+				       DM_BHT_ENTRY_PENDING);
+
+		if (state == DM_BHT_ENTRY_VERIFIED)
+			break;
+		if (state <= DM_BHT_ENTRY_ERROR)
+			goto error_state;
+		if (state != DM_BHT_ENTRY_UNALLOCATED)
+			continue;
+
+		/* Current entry is claimed for allocation and loading */
+		pg = alloc_page(GFP_NOIO);
+		if (!pg)
+			goto nomem;
+
+		/* dm-bht guarantees page-aligned memory for callbacks. */
+		entry->nodes = page_address(pg);
+
+		/* TODO(wad) error check callback here too */
+
+		level = &bht->levels[depth];
+		index = dm_bht_index_at_level(bht, depth, block);
+		bht->read_cb(ctx, level->sector + to_sector(index * PAGE_SIZE),
+			     entry->nodes, to_sector(PAGE_SIZE), entry);
+	}
+
+	return 0;
+
+error_state:
+	DMCRIT("block %u at depth %d is in an error state", block, depth);
+	return state;
+
+nomem:
+	DMCRIT("failed to allocate memory for entry->nodes");
+	return -ENOMEM;
+}
+EXPORT_SYMBOL(dm_bht_populate);
+
+
+/**
+ * dm_bht_verify_block - checks that all nodes in the path for @block are valid
+ * @bht:	pointer to a dm_bht_create()d bht
+ * @block:	specific block data is expected from
+ * @pg:		page holding the block data
+ * @offset:	offset into the page
+ *
+ * Returns 0 on success, 1 on missing data, and a negative error
+ * code on verification failure. All supporting functions called
+ * should return similarly.
+ */
+int dm_bht_verify_block(struct dm_bht *bht, unsigned int block,
+			struct page *pg, unsigned int offset)
+{
+	BUG_ON(offset != 0);
+
+	return  dm_bht_verify_path(bht, block, pg, offset);
+}
+EXPORT_SYMBOL(dm_bht_verify_block);
+
+/**
+ * dm_bht_destroy - cleans up all memory used by @bht
+ * @bht:	pointer to a dm_bht_create()d bht
+ *
+ * Returns 0 on success. Does not free @bht itself.
+ */
+int dm_bht_destroy(struct dm_bht *bht)
+{
+	int depth;
+	int cpu = 0;
+
+	depth = bht->depth;
+	while (depth-- != 0) {
+		struct dm_bht_entry *entry = bht->levels[depth].entries;
+		struct dm_bht_entry *entry_end = entry +
+						 bht->levels[depth].count;
+		int state = 0;
+		for (; entry < entry_end; ++entry) {
+			state = atomic_read(&entry->state);
+			switch (state) {
+			/* At present, no other states free memory,
+			 * but that will change.
+			 */
+			case DM_BHT_ENTRY_UNALLOCATED:
+				/* Allocated with improper state */
+				BUG_ON(entry->nodes);
+				continue;
+			default:
+				BUG_ON(!entry->nodes);
+				__free_page(virt_to_page(entry->nodes));
+				break;
+			}
+		}
+		kfree(bht->levels[depth].entries);
+		bht->levels[depth].entries = NULL;
+	}
+	kfree(bht->levels);
+	for (cpu = 0; cpu < nr_cpu_ids; ++cpu)
+		if (bht->hash_desc[cpu].tfm)
+			crypto_free_hash(bht->hash_desc[cpu].tfm);
+	return 0;
+}
+EXPORT_SYMBOL(dm_bht_destroy);
+
+/*-----------------------------------------------
+ * Accessors
+ *-----------------------------------------------*/
+
+/**
+ * dm_bht_sectors - return the sectors required on disk
+ * @bht:	pointer to a dm_bht_create()d bht
+ */
+sector_t dm_bht_sectors(const struct dm_bht *bht)
+{
+	return bht->sectors;
+}
+EXPORT_SYMBOL(dm_bht_sectors);
+
+/**
+ * dm_bht_set_read_cb - set read callback
+ * @bht:	pointer to a dm_bht_create()d bht
+ * @read_cb:	callback function used for all read requests by @bht
+ */
+void dm_bht_set_read_cb(struct dm_bht *bht, dm_bht_callback read_cb)
+{
+	bht->read_cb = read_cb;
+}
+EXPORT_SYMBOL(dm_bht_set_read_cb);
+
+/**
+ * dm_bht_set_write_cb - set write callback
+ * @bht:	pointer to a dm_bht_create()d bht
+ * @write_cb:	callback function used for all write requests by @bht
+ */
+void dm_bht_set_write_cb(struct dm_bht *bht, dm_bht_callback write_cb)
+{
+	bht->write_cb = write_cb;
+}
+EXPORT_SYMBOL(dm_bht_set_write_cb);
+
+/**
+ * dm_bht_set_root_hexdigest - sets an unverified root digest hash from hex
+ * @bht:	pointer to a dm_bht_create()d bht
+ * @hexdigest:	array of u8s containing the new digest in binary
+ * Returns non-zero on error.  hexdigest should be NUL terminated.
+ */
+int dm_bht_set_root_hexdigest(struct dm_bht *bht, const u8 *hexdigest)
+{
+	/* Make sure we have at least the bytes expected */
+	if (strnlen((char *)hexdigest, bht->digest_size * 2) !=
+	    bht->digest_size * 2) {
+		DMERR("root digest length does not match hash algorithm");
+		return -1;
+	}
+	dm_bht_hex_to_bin(bht->root_digest, hexdigest, bht->digest_size);
+#ifdef CONFIG_DM_DEBUG
+	DMINFO("Set root digest to %s. Parsed as -> ", hexdigest);
+	dm_bht_log_mismatch(bht, bht->root_digest, bht->root_digest);
+#endif
+	return 0;
+}
+EXPORT_SYMBOL(dm_bht_set_root_hexdigest);
+
+/**
+ * dm_bht_root_hexdigest - returns root digest in hex
+ * @bht:	pointer to a dm_bht_create()d bht
+ * @hexdigest:	u8 array of size @available
+ * @available:	must be bht->digest_size * 2 + 1
+ */
+int dm_bht_root_hexdigest(struct dm_bht *bht, u8 *hexdigest, int available)
+{
+	if (available < 0 ||
+	    ((unsigned int) available) < bht->digest_size * 2 + 1) {
+		DMERR("hexdigest has too few bytes available");
+		return -EINVAL;
+	}
+	dm_bht_bin_to_hex(bht->root_digest, hexdigest, bht->digest_size);
+	return 0;
+}
+EXPORT_SYMBOL(dm_bht_root_hexdigest);
+
+/**
+ * dm_bht_set_salt - sets the salt used, in hex
+ * @bht:      pointer to a dm_bht_create()d bht
+ * @hexsalt:  salt string, as hex; will be zero-padded or truncated to
+ *            DM_BHT_SALT_SIZE * 2 hex digits.
+ */
+void dm_bht_set_salt(struct dm_bht *bht, const char *hexsalt)
+{
+	size_t saltlen = min(strlen(hexsalt) / 2, sizeof(bht->salt));
+	bht->have_salt = true;
+	memset(bht->salt, 0, sizeof(bht->salt));
+	dm_bht_hex_to_bin(bht->salt, (const u8 *)hexsalt, saltlen);
+}
+
+/**
+ * dm_bht_salt - returns the salt used, in hex
+ * @bht:      pointer to a dm_bht_create()d bht
+ * @hexsalt:  buffer to put salt into, of length DM_BHT_SALT_SIZE * 2 + 1.
+ */
+int dm_bht_salt(struct dm_bht *bht, char *hexsalt)
+{
+	if (!bht->have_salt)
+		return -EINVAL;
+	dm_bht_bin_to_hex(bht->salt, (u8 *)hexsalt, sizeof(bht->salt));
+	return 0;
+}
Index: linux-3.3.8/drivers/md/dm-ioctl.c
===================================================================
--- linux-3.3.8.orig/drivers/md/dm-ioctl.c	2012-06-01 07:16:13.000000000 +0000
+++ linux-3.3.8/drivers/md/dm-ioctl.c	2014-02-09 08:02:38.867062960 +0000
@@ -1748,6 +1748,46 @@
 	dm_hash_exit();
 }
 
+
+/**
+ * dm_ioctl_export - Permanently export a mapped device via the ioctl interface
+ * @md: Pointer to mapped_device
+ * @name: Buffer (size DM_NAME_LEN) for name
+ * @uuid: Buffer (size DM_UUID_LEN) for uuid or NULL if not desired
+ */
+int dm_ioctl_export(struct mapped_device *md, const char *name,
+		    const char *uuid)
+{
+	int r = 0;
+	struct hash_cell *hc;
+
+	if (!md) {
+		r = -ENXIO;
+		goto out;
+	}
+
+	/* The name and uuid can only be set once. */
+	mutex_lock(&dm_hash_cells_mutex);
+	hc = dm_get_mdptr(md);
+	mutex_unlock(&dm_hash_cells_mutex);
+	if (hc) {
+		DMERR("%s: already exported", dm_device_name(md));
+		r = -ENXIO;
+		goto out;
+	}
+
+	r = dm_hash_insert(name, uuid, md);
+	if (r) {
+		DMERR("%s: could not bind to '%s'", dm_device_name(md), name);
+		goto out;
+	}
+
+	/* Let udev know we've changed. */
+	dm_kobject_uevent(md, KOBJ_CHANGE, dm_get_event_nr(md));
+out:
+	return r;
+}
+
 /**
  * dm_copy_name_and_uuid - Copy mapped device name & uuid into supplied buffers
  * @md: Pointer to mapped_device
Index: linux-3.3.8/drivers/md/dm-verity.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.3.8/drivers/md/dm-verity.c	2014-02-09 08:02:38.867062960 +0000
@@ -0,0 +1,1484 @@
+/*
+ * Originally based on dm-crypt.c,
+ * Copyright (C) 2003 Christophe Saout <christophe@saout.de>
+ * Copyright (C) 2004 Clemens Fruhwirth <clemens@endorphin.org>
+ * Copyright (C) 2006-2008 Red Hat, Inc. All rights reserved.
+ * Copyright (C) 2010 The Chromium OS Authors <chromium-os-dev@chromium.org>
+ *                    All Rights Reserved.
+ *
+ * This file is released under the GPL.
+ *
+ * Implements a verifying transparent block device.
+ * See Documentation/device-mapper/dm-verity.txt
+ */
+#include <linux/async.h>
+#include <linux/bio.h>
+#include <linux/blkdev.h>
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/err.h>
+#include <linux/genhd.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/mempool.h>
+#include <linux/module.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/workqueue.h>
+#include <asm/atomic.h>
+#include <asm/page.h>
+
+/* #define CONFIG_DM_DEBUG 1 */
+#define CONFIG_DM_VERITY_TRACE 1
+#include <linux/device-mapper.h>
+#include <linux/dm-bht.h>
+
+#include "dm-verity.h"
+#include "md.h"
+
+#define DM_MSG_PREFIX "verity"
+
+/* Supports up to 512-bit digests */
+#define VERITY_MAX_DIGEST_SIZE 64
+
+/* TODO(wad) make both of these report the error line/file to a
+ *           verity_bug function.
+ */
+#define VERITY_BUG(msg...) BUG()
+#define VERITY_BUG_ON(cond, msg...) BUG_ON(cond)
+
+/* Helper for printing sector_t */
+#define ULL(x) ((unsigned long long)(x))
+
+/* IOS represent min of dm_verity_ios in a pool, but we also use it to
+ * preallocate biosets (MIN_IOS * 2):
+ * 1. We need to clone the entire bioset, including bio_vecs, before passing
+ *    them to the underlying block layer since it may alter the values.
+ * 2. We need to pad out biosets that are not block aligned.
+ * 3. We need to be able to create biosets while loading in hashes.
+ * This will need more tweaking for specific workload expectations.
+ */
+#define MIN_IOS 32
+/* During io_bht_read, we will spawn _many_ bios for a single I/O early on, but
+ * once the tree is populated, we will only need MIN_IOS at most to be able to
+ * pad out the request. We will also need space for the padding biovecs which
+ * is at most 2, less than one page per side.
+ */
+#define MIN_BIOS (MIN_IOS * 2)
+
+/* MUST be true: SECTOR_SHIFT <= VERITY_BLOCK_SHIFT <= PAGE_SHIFT */
+#define VERITY_BLOCK_SIZE 4096
+#define VERITY_BLOCK_SHIFT 12
+
+/* Support additional tracing of requests */
+#ifdef CONFIG_DM_VERITY_TRACE
+#define VERITY_TRACE(param, fmt, args...) { \
+	if (param) \
+		DMINFO(fmt, ## args); \
+}
+static int request_trace;
+module_param(request_trace, int, 0644);
+MODULE_PARM_DESC(request_trace, "Enable request tracing to DMINFO");
+
+static int alloc_trace;
+module_param(alloc_trace, int, 0644);
+MODULE_PARM_DESC(alloc_trace, "Enable allocation tracing to DMINFO");
+#else
+#define VERITY_TRACE(...)
+#endif
+
+#define REQTRACE(fmt, args...) VERITY_TRACE(request_trace, "req: " fmt, ## args)
+#define ALLOCTRACE(fmt, args...) \
+	VERITY_TRACE(alloc_trace, "alloc: " fmt, ## args)
+
+/* Provide a lightweight means of specifying the global default for
+ * error behavior: eio, reboot, or none
+ * Legacy support for 0 = eio, 1 = reboot/panic, 2 = none, 3 = notify.
+ * This is matched to the enum in dm-verity.h.
+ */
+static const char *allowed_error_behaviors[] = { "eio", "panic", "none",
+						 "notify", NULL };
+static char *error_behavior = "eio";
+module_param(error_behavior, charp, 0644);
+MODULE_PARM_DESC(error_behavior, "Behavior on error "
+				 "(eio, panic, none, notify)");
+
+/* Controls whether verity_get_device will wait forever for a device. */
+static int dev_wait;
+module_param(dev_wait, int, 0444);
+MODULE_PARM_DESC(dev_wait, "Wait forever for a backing device");
+
+/* Used for tracking pending bios as well as for exporting information via
+ * STATUSTYPE_INFO.
+ */
+struct verity_stats {
+	unsigned int io_queue;		/* # pending I/O operations */
+	unsigned int verify_queue;	/* # pending verify operations */
+	unsigned int average_requeues;	/* not implemented */
+
+	/*
+	 * Number of times a data block was ready but we didn't have the hash
+	 * blocks for it yet */
+	unsigned long long total_requeues;
+	unsigned long long total_requests;	/* number of reads */
+
+	unsigned long long total_blocks;	/* total blocks read */
+	unsigned long long total_size;	/* total blocks read */
+
+	unsigned long bht_requests;	/* number of hash blocks read */
+
+	/* number of reads for each block size (log2) */
+	unsigned long io_by_block_size[sizeof(uint64_t) * 8];
+	unsigned long long io_size_by_block_size[sizeof(uint64_t) * 8];
+};
+
+/* per-requested-bio private data */
+enum verity_io_flags {
+	VERITY_IOFLAGS_CLONED = 0x1,	/* original bio has been cloned */
+};
+
+struct dm_verity_io {
+	struct dm_target *target;
+	struct bio *bio;
+	struct delayed_work work;
+	unsigned int flags;
+
+	int error;
+	atomic_t pending;
+
+	sector_t sector;  /* converted to target sector */
+	u64 block;  /* aligned block index */
+	u64 count;  /* aligned count in blocks */
+};
+
+struct verity_config {
+	struct dm_dev *dev;
+	sector_t start;
+	sector_t size;
+
+	struct dm_dev *hash_dev;
+	sector_t hash_start;
+
+	struct dm_bht bht;
+
+	/* Pool required for io contexts */
+	mempool_t *io_pool;
+	/* Pool and bios required for making sure that backing device reads are
+	 * in PAGE_SIZE increments.
+	 */
+	struct bio_set *bs;
+
+	char hash_alg[CRYPTO_MAX_ALG_NAME];
+
+	int error_behavior;
+
+	struct verity_stats stats;
+	const char *name;		/* name for this config */
+	struct dentry *debugfs_dir;	/* debugfs dir for this config */
+};
+
+static struct kmem_cache *_verity_io_pool;
+static struct workqueue_struct *kveritydq, *kverityd_ioq;
+static struct dentry *debugfs_root; /* top-level debugfs dir for verity */
+
+static void kverityd_verify(struct work_struct *work);
+static void kverityd_io(struct work_struct *work);
+static void kverityd_io_bht_populate(struct dm_verity_io *io);
+static void kverityd_io_bht_populate_end(struct bio *, int error);
+
+static BLOCKING_NOTIFIER_HEAD(verity_error_notifier);
+
+/*-----------------------------------------------
+ * Statistic tracking functions
+ *-----------------------------------------------*/
+
+void verity_stats_io_queue_inc(struct verity_config *vc)
+{
+	vc->stats.io_queue++;
+}
+
+void verity_stats_verify_queue_inc(struct verity_config *vc)
+{
+	vc->stats.verify_queue++;
+}
+
+void verity_stats_io_queue_dec(struct verity_config *vc)
+{
+	vc->stats.io_queue--;
+}
+
+void verity_stats_verify_queue_dec(struct verity_config *vc)
+{
+	vc->stats.verify_queue--;
+}
+
+void verity_stats_total_requeues_inc(struct verity_config *vc)
+{
+	vc->stats.total_requeues++;
+}
+
+void verity_stats_total_requests_inc(struct verity_config *vc)
+{
+	vc->stats.total_requests++;
+}
+
+void verity_stats_average_requeues(struct verity_config *vc, int requeues)
+{
+	/* TODO(wad) */
+}
+
+/*-----------------------------------------------
+ * Exported interfaces
+ *-----------------------------------------------*/
+
+int dm_verity_register_error_notifier(struct notifier_block *nb)
+{
+	return blocking_notifier_chain_register(&verity_error_notifier, nb);
+}
+EXPORT_SYMBOL_GPL(dm_verity_register_error_notifier);
+
+int dm_verity_unregister_error_notifier(struct notifier_block *nb)
+{
+	return blocking_notifier_chain_unregister(&verity_error_notifier, nb);
+}
+EXPORT_SYMBOL_GPL(dm_verity_unregister_error_notifier);
+
+/*-----------------------------------------------
+ * Allocation and utility functions
+ *-----------------------------------------------*/
+
+static void kverityd_src_io_read_end(struct bio *clone, int error);
+
+/* Shared destructor for all internal bios */
+static void dm_verity_bio_destructor(struct bio *bio)
+{
+	struct dm_verity_io *io = bio->bi_private;
+	struct verity_config *vc = io->target->private;
+	bio_free(bio, vc->bs);
+}
+
+struct bio *verity_alloc_bioset(struct verity_config *vc, gfp_t gfp_mask,
+				int nr_iovecs)
+{
+	return bio_alloc_bioset(gfp_mask, nr_iovecs, vc->bs);
+}
+
+static struct dm_verity_io *verity_io_alloc(struct dm_target *ti,
+					    struct bio *bio, sector_t sector)
+{
+	struct verity_config *vc = ti->private;
+	struct dm_verity_io *io;
+
+	ALLOCTRACE("dm_verity_io for sector %llu", ULL(sector));
+	io = mempool_alloc(vc->io_pool, GFP_NOIO);
+	if (unlikely(!io))
+		return NULL;
+	io->flags = 0;
+	io->target = ti;
+	io->bio = bio;
+	io->sector = sector;
+	io->error = 0;
+
+	/* Adjust the sector by the virtual starting sector */
+	io->block = (to_bytes(sector)) >> VERITY_BLOCK_SHIFT;
+	io->count = bio->bi_size >> VERITY_BLOCK_SHIFT;
+
+	DMDEBUG("io_alloc for %llu blocks starting at %llu",
+		ULL(io->count), ULL(io->block));
+
+	atomic_set(&io->pending, 0);
+
+	return io;
+}
+
+static struct bio *verity_bio_clone(struct dm_verity_io *io)
+{
+	struct verity_config *vc = io->target->private;
+	struct bio *bio = io->bio;
+	struct bio *clone = verity_alloc_bioset(vc, GFP_NOIO, bio->bi_max_vecs);
+
+	if (!clone)
+		return NULL;
+
+	__bio_clone(clone, bio);
+	clone->bi_private = io;
+	clone->bi_end_io  = kverityd_src_io_read_end;
+	clone->bi_bdev    = vc->dev->bdev;
+	clone->bi_sector  = vc->start + io->sector;
+	clone->bi_destructor = dm_verity_bio_destructor;
+
+	return clone;
+}
+
+/* If the request is not successful, this handler takes action.
+ * TODO make this call a registered handler.
+ */
+static void verity_error(struct verity_config *vc, struct dm_verity_io *io,
+			 int error)
+{
+	const char *message;
+	int error_behavior = DM_VERITY_ERROR_BEHAVIOR_PANIC;
+	dev_t devt = 0;
+	u64 block = ~0;
+	int transient = 1;
+	struct dm_verity_error_state error_state;
+
+	if (vc) {
+		devt = vc->dev->bdev->bd_dev;
+		error_behavior = vc->error_behavior;
+	}
+
+	if (io) {
+		io->error = -EIO;
+		block = io->block;
+	}
+
+	switch (error) {
+	case -ENOMEM:
+		message = "out of memory";
+		break;
+	case -EBUSY:
+		message = "pending data seen during verify";
+		break;
+	case -EFAULT:
+		message = "crypto operation failure";
+		break;
+	case -EACCES:
+		message = "integrity failure";
+		/* Image is bad. */
+		transient = 0;
+		break;
+	case -EPERM:
+		message = "hash tree population failure";
+		/* Should be dm-bht specific errors */
+		transient = 0;
+		break;
+	case -EINVAL:
+		message = "unexpected missing/invalid data";
+		/* The device was configured incorrectly - fallback. */
+		transient = 0;
+		break;
+	default:
+		/* Other errors can be passed through as IO errors */
+		message = "unknown or I/O error";
+		return;
+	}
+
+	DMERR_LIMIT("verification failure occurred: %s", message);
+
+	if (error_behavior == DM_VERITY_ERROR_BEHAVIOR_NOTIFY) {
+		error_state.code = error;
+		error_state.transient = transient;
+		error_state.block = block;
+		error_state.message = message;
+		error_state.dev_start = vc->start;
+		error_state.dev_len = vc->size;
+		error_state.dev = vc->dev->bdev;
+		error_state.hash_dev_start = vc->hash_start;
+		error_state.hash_dev_len = dm_bht_sectors(&vc->bht);
+		error_state.hash_dev = vc->hash_dev->bdev;
+
+		/* Set default fallthrough behavior. */
+		error_state.behavior = DM_VERITY_ERROR_BEHAVIOR_PANIC;
+		error_behavior = DM_VERITY_ERROR_BEHAVIOR_PANIC;
+
+		if (!blocking_notifier_call_chain(
+		    &verity_error_notifier, transient, &error_state)) {
+			error_behavior = error_state.behavior;
+		}
+	}
+
+	switch (error_behavior) {
+	case DM_VERITY_ERROR_BEHAVIOR_EIO:
+		break;
+	case DM_VERITY_ERROR_BEHAVIOR_NONE:
+		if (error != -EIO && io)
+			io->error = 0;
+		break;
+	default:
+		goto do_panic;
+	}
+	return;
+
+do_panic:
+	panic("dm-verity failure: "
+	      "device:%u:%u error:%d block:%llu message:%s",
+	      MAJOR(devt), MINOR(devt), error, ULL(block), message);
+}
+
+/**
+ * verity_parse_error_behavior - parse a behavior charp to the enum
+ * @behavior:	NUL-terminated char array
+ *
+ * Checks if the behavior is valid either as text or as an index digit
+ * and returns the proper enum value or -1 on error.
+ */
+static int verity_parse_error_behavior(const char *behavior)
+{
+	const char **allowed = allowed_error_behaviors;
+	char index = '0';
+
+	for (; *allowed; allowed++, index++)
+		if (!strcmp(*allowed, behavior) || behavior[0] == index)
+			break;
+
+	if (!*allowed)
+		return -1;
+
+	/* Convert to the integer index matching the enum. */
+	return allowed - allowed_error_behaviors;
+}
+
+
+/**
+ * match_dev_by_uuid - callback for finding a partition using its uuid
+ * @dev:	device passed in by the caller
+ * @data:	opaque pointer to a uuid packed by part_pack_uuid().
+ *
+ * Returns 1 if the device matches, and 0 otherwise.
+ */
+static int match_dev_by_uuid(struct device *dev, void *data)
+{
+	u8 *uuid = data;
+	struct hd_struct *part = dev_to_part(dev);
+
+	if (!part->info)
+		goto no_match;
+
+	if (memcmp(uuid, part->info->uuid, sizeof(part->info->uuid)))
+			goto no_match;
+
+	return 1;
+no_match:
+	return 0;
+}
+
+/**
+ * dm_get_device_by_uuid: claim a device using its UUID
+ * @ti:			current dm_target
+ * @uuid_string:	36 byte UUID hex encoded
+ * 			(xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx)
+ * @dev_start:		offset in sectors passed to dm_get_device
+ * @dev_len:		length in sectors passed to dm_get_device
+ * @dm_dev:		dm_dev to populate
+ *
+ * Wraps dm_get_device allowing it to use a unique partition id to
+ * find a given partition on any drive. This code is based on
+ * printk_all_partitions in that it walks all of the register block devices.
+ *
+ * N.B., uuid_string is not checked for safety just strlen().
+ */
+static int dm_get_device_by_uuid(struct dm_target *ti, const char *uuid_str,
+			     sector_t dev_start, sector_t dev_len,
+			     struct dm_dev **dm_dev)
+{
+	struct device *dev = NULL;
+	dev_t devt = 0;
+	char devt_buf[BDEVT_SIZE];
+	u8 uuid[16];
+	size_t uuid_length = strlen(uuid_str);
+
+	if (uuid_length < 36)
+		goto bad_uuid;
+	/* Pack the requested UUID in the expected format. */
+	part_pack_uuid(uuid_str, uuid);
+
+	dev = class_find_device(&block_class, NULL, uuid, &match_dev_by_uuid);
+	if (!dev)
+		goto found_nothing;
+
+	devt = dev->devt;
+	put_device(dev);
+
+	/* The caller may specify +/-%u after the UUID if they want a partition
+	 * before or after the one identified.
+	 */
+	if (uuid_length > 36) {
+		unsigned int part_offset;
+		char sign;
+		unsigned minor = MINOR(devt);
+		if (sscanf(uuid_str + 36, "%c%u", &sign, &part_offset) == 2) {
+			if (sign == '+') {
+				minor += part_offset;
+			} else if (sign == '-') {
+				minor -= part_offset;
+			} else {
+				DMWARN("Trailing characters after UUID: %s\n",
+					uuid_str);
+			}
+			devt = MKDEV(MAJOR(devt), minor);
+		}
+	}
+
+	/* Construct the dev name to pass to dm_get_device.  dm_get_device
+	 * doesn't support being passed a dev_t.
+	 */
+	snprintf(devt_buf, sizeof(devt_buf), "%u:%u", MAJOR(devt), MINOR(devt));
+
+	/* TODO(wad) to make this generic we could also pass in the mode. */
+	if (!dm_get_device(ti, devt_buf, dm_table_get_mode(ti->table), dm_dev))
+		return 0;
+
+	ti->error = "Failed to acquire device";
+	DMDEBUG("Failed to acquire discovered device %s", devt_buf);
+	return -1;
+bad_uuid:
+	ti->error = "Bad UUID";
+	DMDEBUG("Supplied value '%s' is an invalid UUID", uuid_str);
+	return -1;
+found_nothing:
+	DMDEBUG("No matching partition for GUID: %s", uuid_str);
+	ti->error = "No matching GUID";
+	return -1;
+}
+
+static int verity_get_device(struct dm_target *ti, const char *devname,
+			     sector_t dev_start, sector_t dev_len,
+			     struct dm_dev **dm_dev)
+{
+	do {
+		/* Try the normal path first since if everything is ready, it
+		 * will be the fastest.
+		 */
+		if (!dm_get_device(ti, devname,
+				   dm_table_get_mode(ti->table), dm_dev))
+			return 0;
+
+		/* Try the device by partition UUID */
+		if (!dm_get_device_by_uuid(ti, devname, dev_start, dev_len,
+					   dm_dev))
+			return 0;
+
+		/* No need to be too aggressive since this is a slow path. */
+		msleep(500);
+	} while (dev_wait && (driver_probe_done() != 0 || *dm_dev == NULL));
+	async_synchronize_full();
+	return -1;
+}
+
+
+/*-----------------------------------------------------------------
+ * Reverse flow of requests into the device.
+ *
+ * (Start at the bottom with verity_map and work your way upward).
+ *-----------------------------------------------------------------*/
+
+static void verity_inc_pending(struct dm_verity_io *io);
+
+static void verity_return_bio_to_caller(struct dm_verity_io *io)
+{
+	struct verity_config *vc = io->target->private;
+
+	if (io->error)
+		verity_error(vc, io, io->error);
+
+	bio_endio(io->bio, io->error);
+	mempool_free(io, vc->io_pool);
+}
+
+/* Check for any missing bht hashes. */
+static bool verity_is_bht_populated(struct dm_verity_io *io)
+{
+	struct verity_config *vc = io->target->private;
+	u64 block;
+
+	for (block = io->block; block < io->block + io->count; ++block)
+		if (!dm_bht_is_populated(&vc->bht, block))
+			return false;
+
+	return true;
+}
+
+/* verity_dec_pending manages the lifetime of all dm_verity_io structs.
+ * Non-bug error handling is centralized through this interface and
+ * all passage from workqueue to workqueue.
+ */
+static void verity_dec_pending(struct dm_verity_io *io)
+{
+	struct verity_config *vc = io->target->private;
+	VERITY_BUG_ON(!io, "NULL argument");
+
+	DMDEBUG("dec pending %p: %d--", io, atomic_read(&io->pending));
+
+	if (!atomic_dec_and_test(&io->pending))
+		goto done;
+
+	if (unlikely(io->error))
+		goto io_error;
+
+	/* I/Os that were pending may now be ready */
+	if (verity_is_bht_populated(io)) {
+		verity_stats_io_queue_dec(vc);
+		verity_stats_verify_queue_inc(vc);
+		INIT_DELAYED_WORK(&io->work, kverityd_verify);
+		queue_delayed_work(kveritydq, &io->work, 0);
+		REQTRACE("Block %llu+ is being queued for verify (io:%p)",
+			 ULL(io->block), io);
+	} else {
+		INIT_DELAYED_WORK(&io->work, kverityd_io);
+		queue_delayed_work(kverityd_ioq, &io->work, HZ/10);
+		verity_stats_total_requeues_inc(vc);
+		REQTRACE("Block %llu+ is being requeued for io (io:%p)",
+			 ULL(io->block), io);
+	}
+
+done:
+	return;
+
+io_error:
+	verity_return_bio_to_caller(io);
+}
+
+/* Walks the data set and computes the hash of the data read from the
+ * untrusted source device.  The computed hash is then passed to dm-bht
+ * for verification.
+ */
+static int verity_verify(struct verity_config *vc,
+			 struct bio *bio)
+{
+	unsigned int idx;
+	u64 block;
+	int r;
+
+	VERITY_BUG_ON(bio == NULL);
+
+	block = to_bytes(bio->bi_sector) >> VERITY_BLOCK_SHIFT;
+
+	for (idx = bio->bi_idx; idx < bio->bi_vcnt; idx++) {
+		struct bio_vec *bv = bio_iovec_idx(bio, idx);
+
+		VERITY_BUG_ON(bv->bv_offset % VERITY_BLOCK_SIZE);
+		VERITY_BUG_ON(bv->bv_len % VERITY_BLOCK_SIZE);
+
+		DMDEBUG("Updating hash for block %llu", ULL(block));
+
+		/* TODO(msb) handle case where multiple blocks fit in a page */
+		r = dm_bht_verify_block(&vc->bht, block,
+					bv->bv_page, bv->bv_offset);
+		/* dm_bht functions aren't expected to return errno friendly
+		 * values.  They are converted here for uniformity.
+		 */
+		if (r > 0) {
+			DMERR("Pending data for block %llu seen at verify",
+			      ULL(block));
+			r = -EBUSY;
+			goto bad_state;
+		}
+		if (r < 0) {
+			DMERR_LIMIT("Block hash does not match!");
+			r = -EACCES;
+			goto bad_match;
+		}
+		REQTRACE("Block %llu verified", ULL(block));
+
+		block++;
+		/* After completing a block, allow a reschedule.
+		 * TODO(wad) determine if this is truly needed.
+		 */
+		cond_resched();
+	}
+
+	return 0;
+
+bad_state:
+bad_match:
+	return r;
+}
+
+/* Services the verify workqueue */
+static void kverityd_verify(struct work_struct *work)
+{
+	struct delayed_work *dwork = container_of(work, struct delayed_work,
+						  work);
+	struct dm_verity_io *io = container_of(dwork, struct dm_verity_io,
+					       work);
+	struct verity_config *vc = io->target->private;
+
+	io->error = verity_verify(vc, io->bio);
+
+	/* Free up the bio and tag with the return value */
+	verity_stats_verify_queue_dec(vc);
+	verity_return_bio_to_caller(io);
+}
+
+/* Asynchronously called upon the completion of dm-bht I/O.  The status
+ * of the operation is passed back to dm-bht and the next steps are
+ * decided by verity_dec_pending.
+ */
+static void kverityd_io_bht_populate_end(struct bio *bio, int error)
+{
+	struct dm_bht_entry *entry = (struct dm_bht_entry *) bio->bi_private;
+	struct dm_verity_io *io = (struct dm_verity_io *) entry->io_context;
+
+	DMDEBUG("kverityd_io_bht_populate_end (io:%p, entry:%p)", io, entry);
+	/* Tell the tree to atomically update now that we've populated
+	 * the given entry.
+	 */
+	dm_bht_read_completed(entry, error);
+
+	/* Clean up for reuse when reading data to be checked */
+	bio->bi_vcnt = 0;
+	bio->bi_io_vec->bv_offset = 0;
+	bio->bi_io_vec->bv_len = 0;
+	bio->bi_io_vec->bv_page = NULL;
+	/* Restore the private data to I/O so the destructor can be shared. */
+	bio->bi_private = (void *) io;
+	bio_put(bio);
+
+	/* We bail but assume the tree has been marked bad. */
+	if (unlikely(error)) {
+		DMERR("Failed to read for sector %llu (%u)",
+		      ULL(io->bio->bi_sector), io->bio->bi_size);
+		io->error = error;
+		/* Pass through the error to verity_dec_pending below */
+	}
+	/* When pending = 0, it will transition to reading real data */
+	verity_dec_pending(io);
+}
+
+/* Called by dm-bht (via dm_bht_populate), this function provides
+ * the message digests to dm-bht that are stored on disk.
+ */
+static int kverityd_bht_read_callback(void *ctx, sector_t start, u8 *dst,
+				      sector_t count,
+				      struct dm_bht_entry *entry)
+{
+	struct dm_verity_io *io = ctx;  /* I/O for this batch */
+	struct verity_config *vc;
+	struct bio *bio;
+	/* Explicitly catches these so we can use a custom bug route */
+	VERITY_BUG_ON(!io || !dst || !io->target || !io->target->private);
+	VERITY_BUG_ON(!entry);
+	VERITY_BUG_ON(count != to_sector(VERITY_BLOCK_SIZE));
+
+	vc = io->target->private;
+
+	/* The I/O context is nested inside the entry so that we don't need one
+	 * io context per page read.
+	 */
+	entry->io_context = ctx;
+
+	/* We should only get page size requests at present. */
+	verity_inc_pending(io);
+	bio = verity_alloc_bioset(vc, GFP_NOIO, 1);
+	if (unlikely(!bio)) {
+		DMCRIT("Out of memory at bio_alloc_bioset");
+		dm_bht_read_completed(entry, -ENOMEM);
+		return -ENOMEM;
+	}
+	bio->bi_private = (void *) entry;
+	bio->bi_idx = 0;
+	bio->bi_size = VERITY_BLOCK_SIZE;
+	bio->bi_sector = vc->hash_start + start;
+	bio->bi_bdev = vc->hash_dev->bdev;
+	bio->bi_end_io = kverityd_io_bht_populate_end;
+	bio->bi_rw = REQ_META;
+	/* Only need to free the bio since the page is managed by bht */
+	bio->bi_destructor = dm_verity_bio_destructor;
+	bio->bi_vcnt = 1;
+	bio->bi_io_vec->bv_offset = 0;
+	bio->bi_io_vec->bv_len = to_bytes(count);
+	/* dst is guaranteed to be a page_pool allocation */
+	bio->bi_io_vec->bv_page = virt_to_page(dst);
+	/* Track that this I/O is in use.  There should be no risk of the io
+	 * being removed prior since this is called synchronously.
+	 */
+	DMDEBUG("Submitting bht io %p (entry:%p)", io, entry);
+	vc->stats.bht_requests++;
+	generic_make_request(bio);
+	return 0;
+}
+
+/* Submits an io request for each missing block of block hashes.
+ * The last one to return will then enqueue this on the io workqueue.
+ */
+static void kverityd_io_bht_populate(struct dm_verity_io *io)
+{
+	struct verity_config *vc = io->target->private;
+	u64 block;
+
+	REQTRACE("populating %llu starting at block %llu (io:%p)",
+		 ULL(io->count), ULL(io->block), io);
+	for (block = io->block; block < io->block + io->count; ++block) {
+		int populated;
+
+		DMDEBUG("Calling dm_bht_populate for %ull (io:%p)",
+			ULL(block), io);
+		populated = dm_bht_populate(&vc->bht, io, block);
+		if (populated < 0) {
+			DMCRIT("dm_bht_populate error: block %llu (io:%p): %d",
+			       ULL(block), io, populated);
+			/* TODO(wad) support propagating transient errors
+			 *           cleanly.
+			 */
+			/* verity_dec_pending will handle the error case. */
+			io->error = -EPERM;
+			break;
+		}
+	}
+	REQTRACE("Block %llu+ initiated %d requests (io: %p)",
+		 ULL(io->block), atomic_read(&io->pending) - 1, io);
+}
+
+/* Asynchronously called upon the completion of I/O issued
+ * from kverityd_src_io_read. verity_dec_pending() acts as
+ * the scheduler/flow manager.
+ */
+static void kverityd_src_io_read_end(struct bio *clone, int error)
+{
+	struct dm_verity_io *io = clone->bi_private;
+
+	DMDEBUG("I/O completed");
+	if (unlikely(!bio_flagged(clone, BIO_UPTODATE) && !error))
+		error = -EIO;
+
+	if (unlikely(error)) {
+		DMERR("Error occurred: %d (%llu, %u)",
+			error, ULL(clone->bi_sector), clone->bi_size);
+		io->error = error;
+	}
+
+	/* Release the clone which just avoids the block layer from
+	 * leaving offsets, etc in unexpected states.
+	 */
+	bio_put(clone);
+
+	verity_dec_pending(io);
+	DMDEBUG("all data has been loaded from the data device");
+}
+
+/* If not yet underway, an I/O request will be issued to the vc->dev
+ * device for the data needed. It is cloned to avoid unexpected changes
+ * to the original bio struct.
+ */
+static void kverityd_src_io_read(struct dm_verity_io *io)
+{
+	struct verity_config *vc = io->target->private;
+	struct bio *clone;
+
+	VERITY_BUG_ON(!io);
+
+	/* If clone is non-NULL, then the read is already issued. Could also
+	 * check BIO_UPTODATE, but it doesn't seem needed.
+	 */
+	if (io->flags & VERITY_IOFLAGS_CLONED) {
+		DMDEBUG("io_read called with existing bio. bailing: %p", io);
+		return;
+	}
+	io->flags |= VERITY_IOFLAGS_CLONED;
+
+	DMDEBUG("kverity_io_read started");
+
+	/* Clone the bio. The block layer may modify the bvec array. */
+	DMDEBUG("Creating clone of the request");
+	ALLOCTRACE("clone for io %p, sector %llu",
+		   io, ULL(vc->start + io->sector));
+	clone = verity_bio_clone(io);
+	if (unlikely(!clone)) {
+		io->error = -ENOMEM;
+		return;
+	}
+
+	verity_inc_pending(io);
+
+	/* Submit to the block device */
+	DMDEBUG("Submitting bio");
+	/* XXX: check queue_max_hw_sectors(bdev_get_queue(clone->bi_bdev)); */
+	generic_make_request(clone);
+}
+
+/* kverityd_io services the I/O workqueue. For each pass through
+ * the I/O workqueue, a call to populate both the origin drive
+ * data and the hash tree data is made.
+ */
+static void kverityd_io(struct work_struct *work)
+{
+	struct delayed_work *dwork = container_of(work, struct delayed_work,
+						  work);
+	struct dm_verity_io *io = container_of(dwork, struct dm_verity_io,
+					       work);
+	VERITY_BUG_ON(!io->bio);
+
+	/* Issue requests asynchronously. */
+	verity_inc_pending(io);
+	kverityd_src_io_read(io);
+	kverityd_io_bht_populate(io);
+	verity_dec_pending(io);
+}
+
+/* Paired with verity_dec_pending, the pending value in the io dictate the
+ * lifetime of a request and when it is ready to be processed on the
+ * workqueues.
+ */
+static void verity_inc_pending(struct dm_verity_io *io)
+{
+	atomic_inc(&io->pending);
+}
+
+/* Block-level requests start here. */
+static int verity_map(struct dm_target *ti, struct bio *bio,
+		      union map_info *map_context) {
+	struct dm_verity_io *io;
+	struct verity_config *vc;
+	struct request_queue *r_queue;
+
+	if (unlikely(!ti)) {
+		DMERR("dm_target was NULL");
+		return -EIO;
+	}
+
+	vc = ti->private;
+	r_queue = bdev_get_queue(vc->dev->bdev);
+
+	/* Trace incoming bios */
+	REQTRACE("Got a %s for %llu, %u bytes)",
+		(bio_rw(bio) == WRITE ? "WRITE" :
+		(bio_rw(bio) == READ ? "READ" : "READA")),
+		ULL(bio->bi_sector), bio->bi_size);
+
+	verity_stats_total_requests_inc(vc);
+
+	if (bio_data_dir(bio) == WRITE) {
+		/* If we silently drop writes, then the VFS layer will cache
+		 * the write and persist it in memory. While it doesn't change
+		 * the underlying storage, it still may be contrary to the
+		 * behavior expected by a verified, read-only device.
+		 */
+		DMWARN_LIMIT("write request received. rejecting with -EIO.");
+		verity_error(vc, NULL, -EIO);
+		/* bio_endio(bio, -EIO); */
+		return -EIO;
+	} else {
+		VERITY_BUG_ON(bio->bi_sector % to_sector(VERITY_BLOCK_SIZE));
+		VERITY_BUG_ON(bio->bi_size % VERITY_BLOCK_SIZE);
+
+		/* Queue up the request to be verified */
+		io = verity_io_alloc(ti, bio, bio->bi_sector - ti->begin);
+		if (!io) {
+			DMERR_LIMIT("Failed to allocate and init IO data");
+			return DM_MAPIO_REQUEUE;
+		}
+		verity_stats_io_queue_inc(vc);
+		vc->stats.total_blocks += io->count;
+		vc->stats.io_by_block_size[ilog2(io->count)]++;
+
+		vc->stats.total_size += bio->bi_size;
+		vc->stats.io_size_by_block_size[ilog2(io->count)] +=
+			bio->bi_size;
+		INIT_DELAYED_WORK(&io->work, kverityd_io);
+		queue_delayed_work(kverityd_ioq, &io->work, 0);
+	}
+
+	return DM_MAPIO_SUBMITTED;
+}
+
+static int verity_stats_seq_show(struct seq_file *seq, void *offset)
+{
+	struct verity_config *vc = seq->private;
+	struct verity_stats *stats = &vc->stats;
+	unsigned long long running_total;
+	int i;
+
+	seq_printf(seq, "%d\tI/O queue pending\n", (int)stats->io_queue);
+	seq_printf(seq, "%u\tVerify queue pending\n", stats->verify_queue);
+	seq_printf(seq, "%lu\tHash block requests\n", stats->bht_requests);
+	seq_printf(seq, "%llu\tTotal re-queues\n", stats->total_requeues);
+	seq_printf(seq, "%llu\tTotal requests\n", stats->total_requests);
+	seq_printf(seq, "%lluMB\tTotal size\n", stats->total_size >> 20);
+	seq_printf(seq, "%llu\tTotal blocks\n", stats->total_blocks);
+	for (running_total = i = 0; i < 30; i++) {
+		if (stats->io_by_block_size[i]) {
+			running_total += stats->io_size_by_block_size[i];
+			seq_printf(seq, "%lu\tRequests of size %u-%u"
+				" (%uKB to %uKB), %lluKB, "
+				"run.tot. = %lluMB\n",
+				stats->io_by_block_size[i],
+				1U << i, (2U << i) - 1,
+				1U << i << VERITY_BLOCK_SHIFT >> 10,
+				((2U << i) - 1) << VERITY_BLOCK_SHIFT >> 10,
+				stats->io_size_by_block_size[i] >> 10,
+				running_total >> 20);
+		}
+	}
+
+	return 0;
+}
+
+static int verity_stats_open_fs(struct inode *inode, struct file *file)
+{
+	return single_open(file, verity_stats_seq_show, inode->i_private);
+}
+
+static const struct file_operations verity_stats_fops = {
+	.owner = THIS_MODULE,
+	.open = verity_stats_open_fs,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int verity_init_debugfs(struct verity_config *vc)
+{
+	struct dentry *dir, *stats;
+
+	dir = debugfs_create_dir(vc->name, debugfs_root);
+	if (!dir)
+		goto cant_create_dir;
+	stats = debugfs_create_file("stats",
+			S_IFREG | S_IRUSR | S_IRGRP | S_IROTH,
+			dir, vc, &verity_stats_fops);
+	if (!stats)
+		goto cant_create_file;
+
+	vc->debugfs_dir = dir;
+	return 0;
+
+cant_create_file:
+	debugfs_remove_recursive(dir);
+cant_create_dir:
+	return -ENODEV;
+}
+
+static void verity_cleanup_debugfs(struct verity_config *vc)
+{
+	debugfs_remove_recursive(vc->debugfs_dir);
+}
+
+static void splitarg(char *arg, char **key, char **val) {
+	*key = strsep(&arg, "=");
+	*val = strsep(&arg, "");
+}
+
+/*
+ * Non-block interfaces and device-mapper specific code
+ */
+
+/**
+ * verity_ctr - Construct a verified mapping
+ * @ti:   Target being created
+ * @argc: Number of elements in argv
+ * @argv: Vector of key-value pairs (see below).
+ *
+ * Accepts the following keys:
+ * @payload:        hashed device
+ * @hashtree:       device hashtree is stored on
+ * @hashstart:      start address of hashes (default 0)
+ * @alg:            hash algorithm
+ * @root_hexdigest: toplevel hash of the tree
+ * @error_behavior: what to do when verification fails [optional]
+ * @salt:           salt, in hex [optional]
+ *
+ * E.g.,
+ * payload=/dev/sda2 hashtree=/dev/sda3 alg=sha256
+ * root_hexdigest=f08aa4a3695290c569eb1b0ac032ae1040150afb527abbeb0a3da33d82fb2c6e
+ *
+ * TODO(wad):
+ * - Boot time addition
+ * - Track block verification to free block_hashes if memory use is a concern
+ * Testing needed:
+ * - Regular slub_debug tracing (on checkins)
+ * - Improper block hash padding
+ * - Improper bundle padding
+ * - Improper hash layout
+ * - Missing padding at end of device
+ * - Improperly sized underlying devices
+ * - Out of memory conditions (make sure this isn't too flaky under high load!)
+ * - Incorrect superhash
+ * - Incorrect block hashes
+ * - Incorrect bundle hashes
+ * - Boot-up read speed; sustained read speeds
+ */
+static int verity_ctr(struct dm_target *ti, unsigned int argc, char **argv)
+{
+	struct verity_config *vc = NULL;
+	int ret = 0;
+	sector_t blocks;
+	const char *payload = NULL;
+	const char *hashtree = NULL;
+	unsigned long hashstart = 0;
+	const char *alg = NULL;
+	const char *root_hexdigest = NULL;
+	const char *dev_error_behavior = error_behavior;
+	const char *hexsalt = NULL;
+	int i;
+
+	if (argc >= 6 && !strchr(argv[3], '=')) {
+		/* Transitional hack - support the old positional-argument format.
+		 * Detect it because it requires specifying an unused arg
+		 * (depth) which does not contain an '='. */
+		unsigned long long tmpull;
+		if (strcmp(argv[3], "0")) {
+			ti->error = "Non-zero depth supplied";
+			return -EINVAL;
+		}
+		if (sscanf(argv[2], "%llu", &tmpull) != 1) {
+			ti->error = "Invalid hash_start supplied";
+			return -EINVAL;
+		}
+		payload = argv[0];
+		hashtree = argv[1];
+		hashstart = tmpull;
+		alg = argv[4];
+		root_hexdigest = argv[5];
+		if (argc > 6)
+			dev_error_behavior = argv[6];
+	} else {
+		for (i = 0; i < argc; ++i) {
+			char *key, *val;
+			DMWARN("Argument %d: '%s'", i, argv[i]);
+			splitarg(argv[i], &key, &val);
+			if (!key) {
+				DMWARN("Bad argument %d: missing key?", i);
+				break;
+			}
+			if (!val) {
+				DMWARN("Bad argument %d='%s': missing value", i, key);
+				break;
+			}
+			if (!strcmp(key, "alg")) {
+				alg = val;
+			} else if (!strcmp(key, "payload")) {
+				payload = val;
+			} else if (!strcmp(key, "hashtree")) {
+				hashtree = val;
+			} else if (!strcmp(key, "root_hexdigest")) {
+				root_hexdigest = val;
+			} else if (!strcmp(key, "hashstart")) {
+				if (strict_strtoul(val, 10, &hashstart)) {
+					ti->error = "Invalid hashstart";
+					return -EINVAL;
+				}
+			} else if (!strcmp(key, "error_behavior")) {
+				dev_error_behavior = val;
+			} else if (!strcmp(key, "salt")) {
+				hexsalt = val;
+			}
+		}
+	}
+
+#define NEEDARG(n) \
+	if (!(n)) { \
+		ti->error = "Missing argument: " #n; \
+		return -EINVAL; \
+	}
+
+	NEEDARG(alg);
+	NEEDARG(payload);
+	NEEDARG(hashtree);
+	NEEDARG(root_hexdigest);
+
+#undef NEEDARG
+
+	/* The device mapper device should be setup read-only */
+	if ((dm_table_get_mode(ti->table) & ~FMODE_READ) != 0) {
+		ti->error = "Must be created readonly.";
+		return -EINVAL;
+	}
+
+	ALLOCTRACE("verity_config");
+	vc = kzalloc(sizeof(*vc), GFP_KERNEL);
+	if (!vc) {
+		/* TODO(wad) if this is called from the setup helper, then we
+		 * catch these errors and do a CrOS specific thing. if not, we
+		 * need to have this call the error handler.
+		 */
+		return -EINVAL;
+	}
+
+	/* For the name, use the payload default with / changed to _ */
+	vc->name = dm_disk(dm_table_get_md(ti->table))->disk_name;
+
+	if (verity_init_debugfs(vc))
+		goto bad_debugfs;
+
+	/* Calculate the blocks from the given device size */
+	vc->size = ti->len;
+	blocks = to_bytes(vc->size) >> VERITY_BLOCK_SHIFT;
+	if (dm_bht_create(&vc->bht, blocks, alg)) {
+		DMERR("failed to create required bht");
+		goto bad_bht;
+	}
+	if (dm_bht_set_root_hexdigest(&vc->bht, root_hexdigest)) {
+		DMERR("root hexdigest error");
+		goto bad_root_hexdigest;
+	}
+	if (hexsalt)
+		dm_bht_set_salt(&vc->bht, hexsalt);
+	dm_bht_set_read_cb(&vc->bht, kverityd_bht_read_callback);
+
+	/* payload: device to verify */
+	vc->start = 0;  /* TODO: should this support a starting offset? */
+	/* We only ever grab the device in read-only mode. */
+	ret = verity_get_device(ti, payload, vc->start, ti->len, &vc->dev);
+	if (ret) {
+		DMERR("Failed to acquire device '%s': %d", payload, ret);
+		ti->error = "Device lookup failed";
+		goto bad_verity_dev;
+	}
+
+	if ((to_bytes(vc->start) % VERITY_BLOCK_SIZE) ||
+	    (to_bytes(vc->size) % VERITY_BLOCK_SIZE)) {
+		ti->error = "Device must be VERITY_BLOCK_SIZE divisble/aligned";
+		goto bad_hash_start;
+	}
+
+	vc->hash_start = (sector_t)hashstart;
+
+	/* hashtree: device with hashes.
+	 * Note, payload == hashtree is okay as long as the size of
+	 *       ti->len passed to device mapper does not include
+	 *       the hashes.
+	 */
+	if (verity_get_device(ti, hashtree, vc->hash_start,
+			      dm_bht_sectors(&vc->bht), &vc->hash_dev)) {
+		ti->error = "Hash device lookup failed";
+		goto bad_hash_dev;
+	}
+
+	/* We leave the validity on the hash device open until the
+	 * next arg.  Then we go ahead and try to read in all the bundle
+	 * hashes which live after the block hashes.  If it fails, then
+	 * the hash offset was wrong.
+	 */
+
+
+	/* arg4: cryptographic digest algorithm */
+	if (snprintf(vc->hash_alg, CRYPTO_MAX_ALG_NAME, "%s", alg) >=
+	    CRYPTO_MAX_ALG_NAME) {
+		ti->error = "Hash algorithm name is too long";
+		goto bad_hash;
+	}
+
+	/* override with optional device-specific error behavior */
+	vc->error_behavior = verity_parse_error_behavior(dev_error_behavior);
+	if (vc->error_behavior == -1) {
+		ti->error = "Bad error_behavior supplied";
+		goto bad_err_behavior;
+	}
+
+	/* TODO: Maybe issues a request on the io queue for block 0? */
+
+	/* Argument processing is done, setup operational data */
+	/* Pool for dm_verity_io objects */
+	ALLOCTRACE("slab pool for io objects");
+	vc->io_pool = mempool_create_slab_pool(MIN_IOS, _verity_io_pool);
+	if (!vc->io_pool) {
+		ti->error = "Cannot allocate verity io mempool";
+		goto bad_slab_pool;
+	}
+
+	/* Allocate the bioset used for request padding */
+	/* TODO(wad) allocate a separate bioset for the first verify maybe */
+	ALLOCTRACE("bioset for I/O reqs");
+	vc->bs = bioset_create(MIN_BIOS, 0);
+	if (!vc->bs) {
+		ti->error = "Cannot allocate verity bioset";
+		goto bad_bs;
+	}
+
+	ti->num_flush_requests = 1;
+	ti->private = vc;
+
+	/* TODO(wad) add device and hash device names */
+	{
+		char hashdev[BDEVNAME_SIZE], vdev[BDEVNAME_SIZE];
+		bdevname(vc->hash_dev->bdev, hashdev);
+		bdevname(vc->dev->bdev, vdev);
+		DMINFO("dev:%s hash:%s [sectors:%llu blocks:%llu]", vdev,
+		       hashdev, ULL(dm_bht_sectors(&vc->bht)), ULL(blocks));
+	}
+	return 0;
+
+bad_bs:
+	mempool_destroy(vc->io_pool);
+bad_slab_pool:
+bad_err_behavior:
+bad_hash:
+	dm_put_device(ti, vc->hash_dev);
+bad_hash_dev:
+bad_hash_start:
+	dm_put_device(ti, vc->dev);
+bad_bht:
+bad_root_hexdigest:
+bad_verity_dev:
+	verity_cleanup_debugfs(vc);
+bad_debugfs:
+	kfree(vc);   /* hash is not secret so no need to zero */
+	return -EINVAL;
+}
+
+static void verity_dtr(struct dm_target *ti)
+{
+	struct verity_config *vc = (struct verity_config *) ti->private;
+
+	DMDEBUG("Destroying bs");
+	bioset_free(vc->bs);
+	DMDEBUG("Destroying io_pool");
+	mempool_destroy(vc->io_pool);
+
+	DMDEBUG("Destroying block hash tree");
+	dm_bht_destroy(&vc->bht);
+
+	DMDEBUG("Putting hash_dev");
+	dm_put_device(ti, vc->hash_dev);
+
+	DMDEBUG("Putting dev");
+	dm_put_device(ti, vc->dev);
+
+	DMDEBUG("Removing debugfs dir");
+	verity_cleanup_debugfs(vc);
+
+	DMDEBUG("Destroying config");
+	kfree(vc);
+}
+
+static int verity_status(struct dm_target *ti, status_type_t type,
+			char *result, unsigned int maxlen) {
+	struct verity_config *vc = (struct verity_config *) ti->private;
+	unsigned int sz = 0;
+	char hashdev[BDEVNAME_SIZE], vdev[BDEVNAME_SIZE];
+	u8 hexdigest[VERITY_MAX_DIGEST_SIZE * 2 + 1] = { 0 };
+
+	dm_bht_root_hexdigest(&vc->bht, hexdigest, sizeof(hexdigest));
+
+	switch (type) {
+	case STATUSTYPE_INFO:
+		DMEMIT("%u %u %u %llu %llu",
+		       vc->stats.io_queue,
+		       vc->stats.verify_queue,
+		       vc->stats.average_requeues,
+		       vc->stats.total_requeues,
+		       vc->stats.total_requests);
+		break;
+
+	case STATUSTYPE_TABLE:
+		bdevname(vc->hash_dev->bdev, hashdev);
+		bdevname(vc->dev->bdev, vdev);
+		DMEMIT("/dev/%s /dev/%s %llu %u %s %s",
+			vdev,
+			hashdev,
+			ULL(vc->hash_start),
+			vc->bht.depth,
+			vc->hash_alg,
+			hexdigest);
+		break;
+	}
+	return 0;
+}
+
+static int verity_merge(struct dm_target *ti, struct bvec_merge_data *bvm,
+		       struct bio_vec *biovec, int max_size)
+{
+	struct verity_config *vc = ti->private;
+	struct request_queue *q = bdev_get_queue(vc->dev->bdev);
+
+	if (!q->merge_bvec_fn)
+		return max_size;
+
+	bvm->bi_bdev = vc->dev->bdev;
+	bvm->bi_sector = vc->start + bvm->bi_sector - ti->begin;
+
+	/* Optionally, this could just return 0 to stick to single pages. */
+	return min(max_size, q->merge_bvec_fn(q, bvm, biovec));
+}
+
+static int verity_iterate_devices(struct dm_target *ti,
+				 iterate_devices_callout_fn fn, void *data)
+{
+	struct verity_config *vc = ti->private;
+
+	return fn(ti, vc->dev, vc->start, ti->len, data);
+}
+
+static void verity_io_hints(struct dm_target *ti,
+			    struct queue_limits *limits)
+{
+	limits->logical_block_size = VERITY_BLOCK_SIZE;
+	limits->physical_block_size = VERITY_BLOCK_SIZE;
+	blk_limits_io_min(limits, VERITY_BLOCK_SIZE);
+}
+
+static struct target_type verity_target = {
+	.name   = "verity",
+	.version = {0, 1, 0},
+	.module = THIS_MODULE,
+	.ctr    = verity_ctr,
+	.dtr    = verity_dtr,
+	.map    = verity_map,
+	.merge  = verity_merge,
+	.status = verity_status,
+	.iterate_devices = verity_iterate_devices,
+	.io_hints = verity_io_hints,
+};
+
+#define VERITY_WQ_FLAGS (WQ_CPU_INTENSIVE|WQ_HIGHPRI)
+
+static int __init dm_verity_init(void)
+{
+	int r = -ENOMEM;
+
+	debugfs_root = debugfs_create_dir("dm-verity", NULL);
+	if (!debugfs_root) {
+		DMERR("failed to create debugfs directory");
+		r = -ENODEV;
+		goto bad_debugfs_dir;
+	}
+
+	_verity_io_pool = KMEM_CACHE(dm_verity_io, 0);
+	if (!_verity_io_pool) {
+		DMERR("failed to allocate pool dm_verity_io");
+		goto bad_io_pool;
+	}
+
+	kverityd_ioq = alloc_workqueue("kverityd_io", VERITY_WQ_FLAGS, 1);
+	if (!kverityd_ioq) {
+		DMERR("failed to create workqueue kverityd_ioq");
+		goto bad_io_queue;
+	}
+
+	kveritydq = alloc_workqueue("kverityd", VERITY_WQ_FLAGS, 1);
+	if (!kveritydq) {
+		DMERR("failed to create workqueue kveritydq");
+		goto bad_verify_queue;
+	}
+
+	r = dm_register_target(&verity_target);
+	if (r < 0) {
+		DMERR("register failed %d", r);
+		goto register_failed;
+	}
+
+	DMINFO("version %u.%u.%u loaded", verity_target.version[0],
+	       verity_target.version[1], verity_target.version[2]);
+
+	return r;
+
+register_failed:
+	destroy_workqueue(kveritydq);
+bad_verify_queue:
+	destroy_workqueue(kverityd_ioq);
+bad_io_queue:
+	kmem_cache_destroy(_verity_io_pool);
+bad_io_pool:
+	debugfs_remove_recursive(debugfs_root);
+bad_debugfs_dir:
+	return r;
+}
+
+static void __exit dm_verity_exit(void)
+{
+	destroy_workqueue(kveritydq);
+	destroy_workqueue(kverityd_ioq);
+
+	dm_unregister_target(&verity_target);
+	kmem_cache_destroy(_verity_io_pool);
+	debugfs_remove_recursive(debugfs_root);
+}
+
+module_init(dm_verity_init);
+module_exit(dm_verity_exit);
+
+MODULE_AUTHOR("The Chromium OS Authors <chromium-os-dev@chromium.org>");
+MODULE_DESCRIPTION(DM_NAME " target for transparent disk integrity checking");
+MODULE_LICENSE("GPL");
Index: linux-3.3.8/drivers/md/dm-verity.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.3.8/drivers/md/dm-verity.h	2014-02-09 08:02:38.871063122 +0000
@@ -0,0 +1,45 @@
+/*
+ * Copyright (C) 2010 The Chromium OS Authors <chromium-os-dev@chromium.org>
+ *                    All Rights Reserved.
+ *
+ * This file is released under the GPL.
+ *
+ * Provide error types for use when creating a custom error handler.
+ * See Documentation/device-mapper/dm-verity.txt
+ */
+#ifndef DM_VERITY_H
+#define DM_VERITY_H
+
+#include <linux/notifier.h>
+
+struct dm_verity_error_state {
+	int code;
+	int transient;  /* Likely to not happen after a reboot */
+	u64 block;
+	const char *message;
+
+	sector_t dev_start;
+	sector_t dev_len;
+	struct block_device *dev;
+
+	sector_t hash_dev_start;
+	sector_t hash_dev_len;
+	struct block_device *hash_dev;
+
+	/* Final behavior after all notifications are completed. */
+	int behavior;
+};
+
+/* This enum must be matched to allowed_error_behaviors in dm-verity.c */
+enum dm_verity_error_behavior {
+	DM_VERITY_ERROR_BEHAVIOR_EIO = 0,
+	DM_VERITY_ERROR_BEHAVIOR_PANIC,
+	DM_VERITY_ERROR_BEHAVIOR_NONE,
+	DM_VERITY_ERROR_BEHAVIOR_NOTIFY
+};
+
+
+int dm_verity_register_error_notifier(struct notifier_block *nb);
+int dm_verity_unregister_error_notifier(struct notifier_block *nb);
+
+#endif  /* DM_VERITY_H */
Index: linux-3.3.8/include/linux/device-mapper.h
===================================================================
--- linux-3.3.8.orig/include/linux/device-mapper.h	2012-06-01 07:16:13.000000000 +0000
+++ linux-3.3.8/include/linux/device-mapper.h	2014-02-09 08:02:38.871063122 +0000
@@ -302,6 +302,12 @@
 void *dm_get_mdptr(struct mapped_device *md);
 
 /*
+ * Export the device via the ioctl interface (uses mdptr).
+ */
+int dm_ioctl_export(struct mapped_device *md, const char *name,
+		    const char *uuid);
+
+/*
  * A device can still be used while suspended, but I/O is deferred.
  */
 int dm_suspend(struct mapped_device *md, unsigned suspend_flags);
Index: linux-3.3.8/init/Makefile
===================================================================
--- linux-3.3.8.orig/init/Makefile	2012-06-01 07:16:13.000000000 +0000
+++ linux-3.3.8/init/Makefile	2014-02-09 08:02:38.887063778 +0000
@@ -14,6 +14,7 @@
 mounts-$(CONFIG_BLK_DEV_RAM)	+= do_mounts_rd.o
 mounts-$(CONFIG_BLK_DEV_INITRD)	+= do_mounts_initrd.o
 mounts-$(CONFIG_BLK_DEV_MD)	+= do_mounts_md.o
+mounts-$(CONFIG_BLK_DEV_DM)	+= do_mounts_dm.o
 
 # dependencies on generated files need to be listed explicitly
 $(obj)/version.o: include/generated/compile.h
Index: linux-3.3.8/init/do_mounts.c
===================================================================
--- linux-3.3.8.orig/init/do_mounts.c	2012-06-01 07:16:13.000000000 +0000
+++ linux-3.3.8/init/do_mounts.c	2014-02-09 08:02:38.895064104 +0000
@@ -521,6 +521,7 @@
 	wait_for_device_probe();
 
 	md_run_setup();
+	dm_run_setup();
 
 	if (saved_root_name[0]) {
 		root_device_name = saved_root_name;
Index: linux-3.3.8/init/do_mounts.h
===================================================================
--- linux-3.3.8.orig/init/do_mounts.h	2012-06-01 07:16:13.000000000 +0000
+++ linux-3.3.8/init/do_mounts.h	2014-02-09 08:02:38.903064432 +0000
@@ -74,3 +74,13 @@
 static inline void md_run_setup(void) {}
 
 #endif
+
+#ifdef CONFIG_BLK_DEV_DM
+
+void dm_run_setup(void);
+
+#else
+
+static inline void dm_run_setup(void) {}
+
+#endif
Index: linux-3.3.8/init/do_mounts_dm.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.3.8/init/do_mounts_dm.c	2014-02-09 08:02:38.907064595 +0000
@@ -0,0 +1,454 @@
+/* do_mounts_dm.c
+ * Copyright (C) 2010 The Chromium OS Authors <chromium-os-dev@chromium.org>
+ *                    All Rights Reserved.
+ * Based on do_mounts_md.c
+ *
+ * This file is released under the GPL.
+ */
+#include <linux/async.h>
+#include <linux/ctype.h>
+#include <linux/device-mapper.h>
+#include <linux/fs.h>
+#include <linux/string.h>
+#include <linux/delay.h>
+
+#include "do_mounts.h"
+
+#define DM_MAX_DEVICES 256
+#define DM_MAX_TARGETS 256
+#define DM_MAX_NAME 32
+#define DM_MAX_UUID 129
+#define DM_NO_UUID "none"
+
+#define DM_MSG_PREFIX "init"
+
+/* Separators used for parsing the dm= argument. */
+#define DM_FIELD_SEP " "
+#define DM_LINE_SEP ","
+#define DM_ANY_SEP DM_FIELD_SEP DM_LINE_SEP
+
+/*
+ * When the device-mapper and any targets are compiled into the kernel
+ * (not a module), one or more device-mappers may be created and used
+ * as the root device at boot time with the parameters given with the
+ * boot line dm=...
+ *
+ * Multiple device-mappers can be stacked specifing the number of
+ * devices. A device can have multiple targets if the the number of
+ * targets is specified.
+ *
+ * TODO(taysom:defect 32847)
+ * In the future, the <num> field will be mandatory.
+ *
+ * <device>        ::= [<num>] <device-mapper>+
+ * <device-mapper> ::= <head> "," <target>+
+ * <head>          ::= <name> <uuid> <mode> [<num>]
+ * <target>        ::= <start> <length> <type> <options> ","
+ * <mode>          ::= "ro" | "rw"
+ * <uuid>          ::= xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx | "none"
+ * <type>          ::= "verity" | "bootcache" | ...
+ *
+ * Example:
+ * 2 vboot none ro 1,
+ *     0 1768000 bootcache
+ *       device=aa55b119-2a47-8c45-946a-5ac57765011f+1
+ *       signature=76e9be054b15884a9fa85973e9cb274c93afadb6
+ *       cache_start=1768000 max_blocks=100000 size_limit=23 max_trace=20000,
+ *   vroot none ro 1,
+ *     0 1740800 verity payload=254:0 hashtree=254:0 hashstart=1740800 alg=sha1
+ *       root_hexdigest=76e9be054b15884a9fa85973e9cb274c93afadb6
+ *       salt=5b3549d54d6c7a3837b9b81ed72e49463a64c03680c47835bef94d768e5646fe
+ *
+ * Notes:
+ *  1. uuid is a label for the device and we set it to "none".
+ *  2. The <num> field will be optional initially and assumed to be 1.
+ *     Once all the scripts that set these fields have been set, it will
+ *     be made mandatory.
+ */
+
+struct dm_setup_target {
+	sector_t begin;
+	sector_t length;
+	char *type;
+	char *params;
+	/* simple singly linked list */
+	struct dm_setup_target *next;
+};
+
+struct dm_device {
+	int minor;
+	int ro;
+	char name[DM_MAX_NAME];
+	char uuid[DM_MAX_UUID];
+	unsigned long num_targets;
+	struct dm_setup_target *target;
+	int target_count;
+	struct dm_device *next;
+};
+
+struct dm_option {
+	char *start;
+	char *next;
+	size_t len;
+	char delim;
+};
+
+static struct {
+	unsigned long num_devices;
+	char *str;
+} dm_setup_args __initdata;
+
+static __initdata int dm_early_setup;
+
+static int __init get_dm_option(struct dm_option *opt, const char *accept)
+{
+	char *str = opt->next;
+	char *endp;
+
+	if (!str)
+		return 0;
+
+	str = skip_spaces(str);
+	opt->start = str;
+	endp = strpbrk(str, accept);
+	if (!endp) {  /* act like strchrnul */
+		opt->len = strlen(str);
+		endp = str + opt->len;
+	} else {
+		opt->len = endp - str;
+	}
+	opt->delim = *endp;
+	if (*endp == 0) {
+		/* Don't advance past the nul. */
+		opt->next = endp;
+	} else {
+		opt->next = endp + 1;
+	}
+	return opt->len != 0;
+}
+
+static int __init dm_setup_cleanup(struct dm_device *devices)
+{
+	struct dm_device *dev = devices;
+
+	while (dev) {
+		struct dm_device *old_dev = dev;
+		struct dm_setup_target *target = dev->target;
+		while (target) {
+			struct dm_setup_target *old_target = target;
+			kfree(target->type);
+			kfree(target->params);
+			target = target->next;
+			kfree(old_target);
+			dev->target_count--;
+		}
+		BUG_ON(dev->target_count);
+		dev = dev->next;
+		kfree(old_dev);
+	}
+	return 0;
+}
+
+static char * __init dm_parse_device(struct dm_device *dev, char *str)
+{
+	struct dm_option opt;
+	size_t len;
+
+	/* Grab the logical name of the device to be exported to udev */
+	opt.next = str;
+	if (!get_dm_option(&opt, DM_FIELD_SEP)) {
+		DMERR("failed to parse device name");
+		goto parse_fail;
+	}
+	len = min(opt.len + 1, sizeof(dev->name));
+	strlcpy(dev->name, opt.start, len);  /* includes nul */
+
+	/* Grab the UUID value or "none" */
+	if (!get_dm_option(&opt, DM_FIELD_SEP)) {
+		DMERR("failed to parse device uuid");
+		goto parse_fail;
+	}
+	len = min(opt.len + 1, sizeof(dev->uuid));
+	strlcpy(dev->uuid, opt.start, len);
+
+	/* Determine if the table/device will be read only or read-write */
+	get_dm_option(&opt, DM_ANY_SEP);
+	if (!strncmp("ro", opt.start, opt.len)) {
+		dev->ro = 1;
+	} else if (!strncmp("rw", opt.start, opt.len)) {
+		dev->ro = 0;
+	} else {
+		DMERR("failed to parse table mode");
+		goto parse_fail;
+	}
+
+	/* Optional number field */
+	/* XXX: The <num> field will be mandatory in the next round */
+	if (opt.delim == DM_FIELD_SEP[0]) {
+		if (!get_dm_option(&opt, DM_LINE_SEP))
+			return NULL;
+		dev->num_targets = simple_strtoul(opt.start, NULL, 10);
+	} else {
+		dev->num_targets = 1;
+	}
+	if (dev->num_targets > DM_MAX_TARGETS) {
+		DMERR("too many targets %lu > %d",
+			dev->num_targets, DM_MAX_TARGETS);
+	}
+	return opt.next;
+
+parse_fail:
+	return NULL;
+}
+
+static char * __init dm_parse_targets(struct dm_device *dev, char *str)
+{
+	struct dm_option opt;
+	struct dm_setup_target **target = &dev->target;
+	unsigned long num_targets = dev->num_targets;
+	unsigned long i;
+
+	/* Targets are defined as per the table format but with a
+	 * comma as a newline separator. */
+	opt.next = str;
+	for (i = 0; i < num_targets; i++) {
+		*target = kzalloc(sizeof(struct dm_setup_target), GFP_KERNEL);
+		if (!*target) {
+			DMERR("failed to allocate memory for target %s<%ld>",
+				dev->name, i);
+			goto parse_fail;
+		}
+		dev->target_count++;
+
+		if (!get_dm_option(&opt, DM_FIELD_SEP)) {
+			DMERR("failed to parse starting sector"
+				" for target %s<%ld>", dev->name, i);
+			goto parse_fail;
+		}
+		(*target)->begin = simple_strtoull(opt.start, NULL, 10);
+
+		if (!get_dm_option(&opt, DM_FIELD_SEP)) {
+			DMERR("failed to parse length for target %s<%ld>",
+				dev->name, i);
+			goto parse_fail;
+		}
+		(*target)->length = simple_strtoull(opt.start, NULL, 10);
+
+		if (get_dm_option(&opt, DM_FIELD_SEP))
+			(*target)->type = kstrndup(opt.start, opt.len,
+							GFP_KERNEL);
+		if (!((*target)->type)) {
+			DMERR("failed to parse type for target %s<%ld>",
+				dev->name, i);
+			goto parse_fail;
+		}
+		if (get_dm_option(&opt, DM_LINE_SEP))
+			(*target)->params = kstrndup(opt.start, opt.len,
+							GFP_KERNEL);
+		if (!((*target)->params)) {
+			DMERR("failed to parse params for target %s<%ld>",
+				dev->name, i);
+			goto parse_fail;
+		}
+		target = &((*target)->next);
+	}
+	DMDEBUG("parsed %d targets", dev->target_count);
+
+	return opt.next;
+
+parse_fail:
+	return NULL;
+}
+
+static struct dm_device *dm_parse_args(void)
+{
+	struct dm_device *devices = NULL;
+	struct dm_device **tail = &devices;
+	struct dm_device *dev;
+	char *str = dm_setup_args.str;
+	unsigned long num_devices = dm_setup_args.num_devices;
+	unsigned long i;
+
+	if (!str)
+		return NULL;
+	for (i = 0; i < num_devices; i++) {
+		dev = kzalloc(sizeof(*dev), GFP_KERNEL);
+		if (!dev) {
+			DMERR("failed to allocated memory for dev");
+			goto error;
+		}
+		*tail = dev;
+		tail = &dev->next;
+		/*
+		 * devices are given minor numbers 0 - n-1
+		 * in the order they are found in the arg
+		 * string.
+		 */
+		dev->minor = i;
+		str = dm_parse_device(dev, str);
+		if (!str)	/* NULL indicates error in parsing, bail */
+			goto error;
+
+		str = dm_parse_targets(dev, str);
+		if (!str)
+			goto error;
+	}
+	return devices;
+error:
+	dm_setup_cleanup(devices);
+	return NULL;
+}
+
+/*
+ * Parse the command-line parameters given our kernel, but do not
+ * actually try to invoke the DM device now; that is handled by
+ * dm_setup_drives after the low-level disk drivers have initialised.
+ * dm format is described at the top of the file.
+ *
+ * Because dm minor numbers are assigned in assending order starting with 0,
+ * You can assume the first device is /dev/dm-0, the next device is /dev/dm-1,
+ * and so forth.
+ */
+static int __init dm_setup(char *str)
+{
+	struct dm_option opt;
+	unsigned long num_devices;
+
+	if (!str) {
+		DMDEBUG("str is NULL");
+		goto parse_fail;
+	}
+	opt.next = str;
+	if (!get_dm_option(&opt, DM_FIELD_SEP))
+		goto parse_fail;
+	if (isdigit(opt.start[0])) {	/* XXX: Optional number field */
+		num_devices = simple_strtoul(opt.start, NULL, 10);
+		str = opt.next;
+	} else {
+		num_devices = 1;
+		/* Don't advance str */
+	}
+	if (num_devices > DM_MAX_DEVICES) {
+		DMDEBUG("too many devices %lu > %d",
+			num_devices, DM_MAX_DEVICES);
+	}
+	dm_setup_args.str = str;
+	dm_setup_args.num_devices = num_devices;
+	DMINFO("will configure %lu devices", num_devices);
+	dm_early_setup = 1;
+	return 1;
+
+parse_fail:
+	DMWARN("Invalid arguments supplied to dm=.");
+	return 0;
+}
+
+static void __init dm_setup_drives(void)
+{
+	struct mapped_device *md = NULL;
+	struct dm_table *table = NULL;
+	struct dm_setup_target *target;
+	struct dm_device *dev;
+	char *uuid;
+	fmode_t fmode = FMODE_READ;
+	struct dm_device *devices;
+
+	devices = dm_parse_args();
+
+	for (dev = devices; dev; dev = dev->next) {
+		if (dm_create(dev->minor, &md)) {
+			DMDEBUG("failed to create the device");
+			goto dm_create_fail;
+		}
+		DMDEBUG("created device '%s'", dm_device_name(md));
+
+		/*
+		 * In addition to flagging the table below, the disk must be
+		 * set explicitly ro/rw.
+		 */
+		set_disk_ro(dm_disk(md), dev->ro);
+
+		if (!dev->ro)
+			fmode |= FMODE_WRITE;
+		if (dm_table_create(&table, fmode, dev->target_count, md)) {
+			DMDEBUG("failed to create the table");
+			goto dm_table_create_fail;
+		}
+		for (target = dev->target; target; target = target->next) {
+			DMINFO("adding target '%llu %llu %s %s'",
+			       (unsigned long long) target->begin,
+			       (unsigned long long) target->length,
+			       target->type, target->params);
+			if (dm_table_add_target(table, target->type,
+						target->begin,
+						target->length,
+						target->params)) {
+				DMDEBUG("failed to add the target"
+					" to the table");
+				goto add_target_fail;
+			}
+		}
+		if (dm_table_complete(table)) {
+			DMDEBUG("failed to complete the table");
+			goto table_complete_fail;
+		}
+
+		/* Suspend the device so that we can bind it to the table. */
+		if (dm_suspend(md, 0)) {
+			DMDEBUG("failed to suspend the device pre-bind");
+			goto suspend_fail;
+		}
+
+		/*
+		 * Bind the table to the device. This is the only way
+		 * to associate md->map with the table and set the disk
+		 * capacity directly.
+		 */
+		if (dm_swap_table(md, table)) {  /* should return NULL. */
+			DMDEBUG("failed to bind the device to the table");
+			goto table_bind_fail;
+		}
+
+		/* Finally, resume and the device should be ready. */
+		if (dm_resume(md)) {
+			DMDEBUG("failed to resume the device");
+			goto resume_fail;
+		}
+
+		/* Export the dm device via the ioctl interface */
+		if (!strcmp(DM_NO_UUID, dev->uuid))
+			uuid = NULL;
+		if (dm_ioctl_export(md, dev->name, uuid)) {
+			DMDEBUG("failed to export device with given"
+				" name and uuid");
+			goto export_fail;
+		}
+		DMINFO("dm-%d is ready", dev->minor);
+	}
+	dm_setup_cleanup(devices);
+	return;
+
+export_fail:
+resume_fail:
+table_bind_fail:
+suspend_fail:
+table_complete_fail:
+add_target_fail:
+	dm_table_put(table);
+dm_table_create_fail:
+	dm_put(md);
+dm_create_fail:
+	DMWARN("starting dm-%d (%s) failed",
+	       dev->minor, dev->name);
+	dm_setup_cleanup(devices);
+}
+
+__setup("dm=", dm_setup);
+
+void __init dm_run_setup(void)
+{
+	if (!dm_early_setup)
+		return;
+	DMINFO("attempting early device configuration.");
+	dm_setup_drives();
+}
Index: linux-3.3.8/include/linux/dm-bht.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.3.8/include/linux/dm-bht.h	2014-02-09 11:53:12.719787409 +0000
@@ -0,0 +1,146 @@
+/*
+ * Copyright (C) 2010 The Chromium OS Authors <chromium-os-dev@chromium.org>
+ *
+ * Device-Mapper block hash tree interface.
+ * See Documentation/device-mapper/dm-bht.txt for details.
+ *
+ * This file is released under the GPLv2.
+ */
+#ifndef __LINUX_DM_BHT_H
+#define __LINUX_DM_BHT_H
+
+#include <linux/compiler.h>
+#include <linux/crypto.h>
+#include <linux/types.h>
+
+/* To avoid allocating memory for digest tests, we just setup a
+ * max to use for now.
+ */
+#define DM_BHT_MAX_DIGEST_SIZE 128  /* 1k hashes are unlikely for now */
+#define DM_BHT_SALT_SIZE       32   /* 256 bits of salt is a lot */
+
+/* UNALLOCATED, PENDING, READY, and VERIFIED are valid states. All other
+ * values are entry-related return codes.
+ */
+#define DM_BHT_ENTRY_VERIFIED 8  /* 'nodes' has been checked against parent */
+#define DM_BHT_ENTRY_READY 4  /* 'nodes' is loaded and available */
+#define DM_BHT_ENTRY_PENDING 2  /* 'nodes' is being loaded */
+#define DM_BHT_ENTRY_UNALLOCATED 0 /* untouched */
+#define DM_BHT_ENTRY_ERROR -1 /* entry is unsuitable for use */
+#define DM_BHT_ENTRY_ERROR_IO -2 /* I/O error on load */
+
+/* Additional possible return codes */
+#define DM_BHT_ENTRY_ERROR_MISMATCH -3 /* Digest mismatch */
+
+/* dm_bht_entry
+ * Contains dm_bht->node_count tree nodes at a given tree depth.
+ * state is used to transactionally assure that data is paged in
+ * from disk.  Unless dm_bht kept running crypto contexts for each
+ * level, we need to load in the data for on-demand verification.
+ */
+struct dm_bht_entry {
+	atomic_t state; /* see defines */
+	/* Keeping an extra pointer per entry wastes up to ~33k of
+	 * memory if a 1m blocks are used (or 66 on 64-bit arch)
+	 */
+	void *io_context;  /* Reserve a pointer for use during io */
+	/* data should only be non-NULL if fully populated. */
+	u8 *nodes;  /* The hash data used to verify the children.
+		     * Guaranteed to be page-aligned.
+		     */
+};
+
+/* dm_bht_level
+ * Contains an array of entries which represent a page of hashes where
+ * each hash is a node in the tree at the given tree depth/level.
+ */
+struct dm_bht_level {
+	struct dm_bht_entry *entries;  /* array of entries of tree nodes */
+	unsigned int count;  /* number of entries at this level */
+	sector_t sector;  /* starting sector for this level */
+};
+
+/* opaque context, start, databuf, sector_count */
+typedef int(*dm_bht_callback)(void *,  /* external context */
+			      sector_t,  /* start sector */
+			      u8 *,  /* destination page */
+			      sector_t,  /* num sectors */
+			      struct dm_bht_entry *);
+/* dm_bht - Device mapper block hash tree
+ * dm_bht provides a fixed interface for comparing data blocks
+ * against a cryptographic hashes stored in a hash tree. It
+ * optimizes the tree structure for storage on disk.
+ *
+ * The tree is built from the bottom up.  A collection of data,
+ * external to the tree, is hashed and these hashes are stored
+ * as the blocks in the tree.  For some number of these hashes,
+ * a parent node is created by hashing them.  These steps are
+ * repeated.
+ *
+ * TODO(wad): All hash storage memory is pre-allocated and freed once an
+ * entire branch has been verified.
+ */
+struct dm_bht {
+	/* Configured values */
+	int depth;  /* Depth of the tree including the root */
+	unsigned int block_count;  /* Number of blocks hashed */
+	char hash_alg[CRYPTO_MAX_ALG_NAME];
+	unsigned char salt[DM_BHT_SALT_SIZE];
+
+	/* This is a temporary hack to ease the transition to salting. It will
+	 * be removed once salting is supported both in kernel and userspace,
+	 * and the salt will default to all zeroes instead. */
+	bool have_salt;
+
+	/* Computed values */
+	unsigned int node_count;  /* Data size (in hashes) for each entry */
+	unsigned int node_count_shift;  /* first bit set - 1 */
+	/* There is one per CPU so that verified can be simultaneous. */
+	struct hash_desc hash_desc[NR_CPUS];  /* Container for the hash alg */
+	unsigned int digest_size;
+	sector_t sectors;  /* Number of disk sectors used */
+
+	/* bool verified;  Full tree is verified */
+	u8 root_digest[DM_BHT_MAX_DIGEST_SIZE];
+	struct dm_bht_level *levels;  /* in reverse order */
+	/* Callbacks for reading and/or writing to the hash device */
+	dm_bht_callback read_cb;
+	dm_bht_callback write_cb;
+};
+
+/* Constructor for struct dm_bht instances. */
+int dm_bht_create(struct dm_bht *bht,
+		  unsigned int block_count,
+		  const char *alg_name);
+/* Destructor for struct dm_bht instances.  Does not free @bht */
+int dm_bht_destroy(struct dm_bht *bht);
+
+/* Basic accessors for struct dm_bht */
+sector_t dm_bht_sectors(const struct dm_bht *bht);
+void dm_bht_set_read_cb(struct dm_bht *bht, dm_bht_callback read_cb);
+void dm_bht_set_write_cb(struct dm_bht *bht, dm_bht_callback write_cb);
+int dm_bht_set_root_hexdigest(struct dm_bht *bht, const u8 *hexdigest);
+int dm_bht_root_hexdigest(struct dm_bht *bht, u8 *hexdigest, int available);
+void dm_bht_set_salt(struct dm_bht *bht, const char *hexsalt);
+int dm_bht_salt(struct dm_bht *bht, char *hexsalt);
+
+/* Functions for loading in data from disk for verification */
+bool dm_bht_is_populated(struct dm_bht *bht, unsigned int block);
+int dm_bht_populate(struct dm_bht *bht, void *read_cb_ctx,
+		    unsigned int block);
+int dm_bht_verify_block(struct dm_bht *bht, unsigned int block,
+			struct page *pg, unsigned int offset);
+
+/* Functions for creating struct dm_bhts on disk.  A newly created dm_bht
+ * should not be directly used for verification. (It should be repopulated.)
+ * In addition, these functions aren't meant to be called in parallel.
+ */
+int dm_bht_compute(struct dm_bht *bht, void *read_cb_ctx);
+int dm_bht_sync(struct dm_bht *bht, void *write_cb_ctx);
+int dm_bht_store_block(struct dm_bht *bht, unsigned int block,
+		       u8 *block_data);
+int dm_bht_zeroread_callback(void *ctx, sector_t start, u8 *dst, sector_t count,
+			     struct dm_bht_entry *entry);
+void dm_bht_read_completed(struct dm_bht_entry *entry, int status);
+void dm_bht_write_completed(struct dm_bht_entry *entry, int status);
+#endif  /* __LINUX_DM_BHT_H */
